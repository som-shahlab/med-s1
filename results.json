{
  "experiments": {
    "base": {
      "description": "Base LLaMA model without fine-tuning",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": null,
        "training_params": null,
        "datasets": {
          "curate": null,
          "eval": "huatuo-eval"
        }
      },
      "results": {
        "curation": null,
        "training": null,
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base/Llama-3.1-8B-Instructeval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base/Llama-3.1-8B-Instructeval_data_metrics.json",
          "timestamp": "2025-03-15T22:16:49.150561",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/base_eval.json"
        }
      }
    },
    "base-step-prompt": {
      "description": "Base model with step-by-step prompting",
      "config": {
        "model_key": "llama3.1:8b",
        "prompting": "step",
        "datasets": {
          "curate": "huatuo-sft",
          "eval": "huatuo-eval"
        }
      },
      "results": {
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base-step-prompt/Llama-3.1-8B-Instructeval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base-step-prompt/Llama-3.1-8B-Instructeval_data_metrics.json",
          "timestamp": "2025-03-15T05:51:41.888151",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/base-step-prompt_eval.json"
        }
      }
    },
    "huatuo": {
      "description": "Pre-trained HuatuoGPT medical model",
      "config": {
        "model_key": "huatuo:8b",
        "curation": null,
        "training_params": null,
        "datasets": "same as base"
      },
      "results": {
        "curation": null,
        "training": null,
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo/HuatuoGPT-o1-8Beval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo/HuatuoGPT-o1-8Beval_data_metrics.json",
          "timestamp": "2025-03-15T22:29:05.214214",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/huatuo_eval.json"
        }
      }
    },
    "medqa-1k-random": {
      "description": "1k uniform random sample",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 10,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random_20250329_021551/med_s1k_formatted",
          "timestamp": "2025-03-29T02:15:53.392216",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random",
          "timestamp": "2025-03-29T05:18:27Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:23:30.544018",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random_eval.json"
        }
      }
    },
    "medqa-1k-embedding-diversity-question-cluster-10-outlier-5": {
      "description": "1k sample using embedding diversity on questions with 10% clusters and 5% outliers",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "embedding-diversity",
          "huatuo_format": true,
          "n_samples": 1000,
          "column": "Question",
          "cluster_percentage": 10,
          "outlier_percentage": 5
        },
        "training_params": "same as medqa-1k-random",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-embedding-diversity-question-cluster-10-outlier-5_20250315_174140/med_s1k_formatted",
          "timestamp": "2025-03-15T17:41:42.398295",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-embedding-diversity-question-cluster-10-outlier-5_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-embedding-diversity-question-cluster-10-outlier-5",
          "timestamp": "2025-03-15T20:40:12Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-embedding-diversity-question-cluster-10-outlier-5/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-embedding-diversity-question-cluster-10-outlier-5/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-15T22:57:04.108404",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-embedding-diversity-question-cluster-10-outlier-5_eval.json"
        }
      }
    },
    "medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5": {
      "description": "1k sample using embedding diversity on questions with 10% clusters and 5% outliers, ranked by CoT length",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "embedding-diversity",
          "huatuo_format": true,
          "n_samples": 1000,
          "column": "Question",
          "cluster_percentage": 10,
          "outlier_percentage": 5,
          "intra_group_ranking": "cot-length"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 8,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5_20250315_174300/med_s1k_formatted",
          "timestamp": "2025-03-15T17:43:02.524140",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
          "timestamp": "2025-03-15T20:52:17Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-15T23:05:18.243880",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5_eval.json"
        }
      }
    },
    "medqa-25k": {
      "description": "Full dataset with all examples",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "all",
          "huatuo_format": true,
          "n_samples": 25371
        },
        "training_params": {
          "learning_rate": 2e-06,
          "batch_size": 2,
          "num_epochs": 3,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-25k_20250331_183859/med_s1k_formatted",
          "timestamp": "2025-03-31T18:39:05.722471",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-25k_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-25k",
          "timestamp": "2025-04-01T00:17:45Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-25k/best_modelMedMCQA_validation.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-25k/best_modelMedMCQA_validation_metrics.json",
          "timestamp": "2025-04-01T01:29:57.671736",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-25k_eval.json"
        }
      }
    },
    "medqa-25k-no-cot": {
      "description": "Full dataset with no CoT",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "all",
          "huatuo_format": true,
          "n_samples": 25371,
          "extract": "none"
        },
        "training_params": {
          "learning_rate": 2e-06,
          "batch_size": 2,
          "num_epochs": 3,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-25k-no-cot_20250331_183859/med_s1k_formatted",
          "timestamp": "2025-03-31T18:39:05.000165",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-25k-no-cot_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-25k-no-cot",
          "timestamp": "2025-03-31T23:26:12Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-25k-no-cot/best_modelMedMCQA_validation.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-25k-no-cot/best_modelMedMCQA_validation_metrics.json",
          "timestamp": "2025-04-01T01:21:59.419127",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-25k-no-cot_eval.json"
        }
      }
    },
    "medqa-25k-step-extract": {
      "description": "Full dataset with step extract",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "all",
          "huatuo_format": true,
          "n_samples": 25371,
          "extract": "step"
        },
        "training_params": {
          "learning_rate": 2e-06,
          "batch_size": 2,
          "num_epochs": 3,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-25k-step-extract_20250331_183859/med_s1k_formatted",
          "timestamp": "2025-03-31T19:21:28.098512",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-25k-step-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-25k-step-extract",
          "timestamp": "2025-04-01T01:13:49Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-25k-step-extract/best_modelMedMCQA_validation.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-25k-step-extract/best_modelMedMCQA_validation_metrics.json",
          "timestamp": "2025-04-01T01:31:44.165002",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-25k-step-extract_eval.json"
        }
      }
    },
    "medqa-1k-random-step-extract": {
      "description": "1k uniform random sample with step extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-step-extract_20250329_021551/med_s1k_formatted",
          "timestamp": "2025-03-29T02:16:46.842031",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-step-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-step-extract",
          "timestamp": "2025-03-29T06:25:00Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-step-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-step-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:25:31.670696",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-step-extract_eval.json"
        }
      }
    },
    "medqa-1k-random-no-cot": {
      "description": "1k uniform random sample without CoT",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "none"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 15,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-no-cot_20250315_174325/med_s1k_formatted",
          "timestamp": "2025-03-15T17:43:27.257948",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-no-cot_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-no-cot",
          "timestamp": "2025-03-15T23:51:24Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-no-cot/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-no-cot/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-15T23:56:47.782596",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-no-cot_eval.json"
        }
      }
    },
    "medqa-1k-random-1-sentence-extract": {
      "description": "1k uniform random sample with 1-sentence CoT extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "1-sentence"
        },
        "training_params": {
          "learning_rate": 1e-05,
          "batch_size": 2,
          "num_epochs": 12,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-1-sentence-extract_20250315_191432/med_s1k_formatted",
          "timestamp": "2025-03-15T19:16:27.998188",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-1-sentence-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-1-sentence-extract",
          "timestamp": "2025-03-18T17:59:26Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-1-sentence-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-1-sentence-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-15T22:53:23.551424",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-1-sentence-extract_eval.json"
        }
      }
    },
    "medqa-5k-random-no-cot": {
      "description": "5k uniform random sample without CoT",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 5000,
          "extract": "none"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 6,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-5k-random-no-cot_20250315_174413/med_s1k_formatted",
          "timestamp": "2025-03-15T17:44:16.109350",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-5k-random-no-cot_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-5k-random-no-cot",
          "timestamp": "2025-03-18T18:13:55Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-5k-random-no-cot/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-5k-random-no-cot/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-20T02:48:45.125184",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-5k-random-no-cot_eval.json"
        }
      }
    },
    "medqa-10k-random-no-cot": {
      "description": "10k uniform random sample without CoT",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 10000,
          "extract": "none"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 4,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-10k-random-no-cot_20250315_173858/med_s1k_formatted",
          "timestamp": "2025-03-15T17:39:01.587844",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-10k-random-no-cot_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-10k-random-no-cot",
          "timestamp": "2025-03-18T18:28:42Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-10k-random-no-cot/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-10k-random-no-cot/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-20T02:48:55.758592",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-10k-random-no-cot_eval.json"
        }
      }
    },
    "medqa-1k-random-note-extract": {
      "description": "1k uniform random sample with auto-determined clinical note format extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "note"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-note-extract_20250329_021551/med_s1k_formatted",
          "timestamp": "2025-03-29T02:17:12.436077",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-note-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-note-extract",
          "timestamp": "2025-03-29T06:57:06Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-note-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-note-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:36:40.380198",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-note-extract_eval.json"
        }
      }
    },
    "medqa-1k-random-list-extract": {
      "description": "1k uniform random sample with bullet-point list extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "list"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-list-extract_20250329_021549/med_s1k_formatted",
          "timestamp": "2025-03-29T02:16:20.762996",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-list-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-list-extract",
          "timestamp": "2025-03-29T05:29:52Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-list-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-list-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:39:55.646475",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-list-extract_eval.json"
        }
      }
    },
    "medqa-1k-random-markdown-extract": {
      "description": "1k uniform random sample with markdown document extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "markdown"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-markdown-extract_20250329_021549/med_s1k_formatted",
          "timestamp": "2025-03-29T02:17:58.043964",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-markdown-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-markdown-extract",
          "timestamp": "2025-03-29T07:13:28Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-markdown-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-markdown-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:39:12.957447",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-markdown-extract_eval.json"
        }
      }
    },
    "medqa-1k-random-evidence-extract": {
      "description": "1k uniform random sample with auto-determined clinical note format extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step-evidence"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-evidence-extract_20250329_021549/med_s1k_formatted",
          "timestamp": "2025-03-29T02:17:07.267179",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-evidence-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-evidence-extract",
          "timestamp": "2025-03-29T06:41:44Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-evidence-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-evidence-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:31:51.177710",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-evidence-extract_eval.json"
        }
      }
    },
    "medqa-1k-random-collapse-33": {
      "description": "1k random sample with 33% collapse consecutive steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "collapse_consecutive",
            "rate": 0.33
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-collapse-33_20250329_105304/med_s1k_formatted",
          "timestamp": "2025-03-29T10:53:45.957687",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-33_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-collapse-33",
          "timestamp": "2025-03-29T11:13:39Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-33/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-33/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:52:50.832745",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-33_eval.json"
        }
      }
    },
    "medqa-1k-random-collapse-66": {
      "description": "1k random sample with 66% collapse consecutive steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "collapse_consecutive",
            "rate": 0.66
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-collapse-66_20250329_105304/med_s1k_formatted",
          "timestamp": "2025-03-29T10:53:54.105800",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-66_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-collapse-66",
          "timestamp": "2025-03-29T11:28:50Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-66/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-66/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:00:29.117394",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-66_eval.json"
        }
      }
    },
    "medqa-1k-random-collapse-100": {
      "description": "1k random sample with 100% collapse consecutive steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "collapse_consecutive",
            "rate": 1.0
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-collapse-100_20250329_105304/med_s1k_formatted",
          "timestamp": "2025-03-29T10:56:32.398973",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-100_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-collapse-100",
          "timestamp": "2025-03-29T11:39:37Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-100/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-100/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:54:55.120295",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-100_eval.json"
        }
      }
    },
    "medqa-1k-random-skip-33": {
      "description": "1k random sample with 33% skip steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "skip",
            "rate": 0.33
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-skip-33_20250329_105304/med_s1k_formatted",
          "timestamp": "2025-03-29T10:53:07.162615",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-33_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-skip-33",
          "timestamp": "2025-03-29T11:50:22Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-33/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-33/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:04:37.797610",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-33_eval.json"
        }
      }
    },
    "medqa-1k-random-skip-66": {
      "description": "1k random sample with 66% skip steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "skip",
            "rate": 0.66
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-skip-66_20250329_104212/med_s1k_formatted",
          "timestamp": "2025-03-29T10:42:16.609707",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-66_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-skip-66",
          "timestamp": "2025-03-29T12:00:59Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-66/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-66/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:02:14.728632",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-66_eval.json"
        }
      }
    },
    "medqa-1k-random-skip-100": {
      "description": "1k random sample with 100% skip steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "skip",
            "rate": 1.0
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-skip-100_20250329_105304/med_s1k_formatted",
          "timestamp": "2025-03-29T10:53:07.492311",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-100_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-skip-100",
          "timestamp": "2025-03-29T12:11:17Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-100/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-100/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:00:16.566462",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-100_eval.json"
        }
      }
    },
    "medqa-1k-random-shuffle-33": {
      "description": "1k random sample with 33% shuffle steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "shuffle",
            "rate": 0.33
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-shuffle-33_20250329_105309/med_s1k_formatted",
          "timestamp": "2025-03-29T10:53:12.719368",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-33_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-shuffle-33",
          "timestamp": "2025-03-29T12:26:55Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-33/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-33/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:16:02.936914",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-33_eval.json"
        }
      }
    },
    "medqa-1k-random-shuffle-66": {
      "description": "1k random sample with 66% shuffle steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "shuffle",
            "rate": 0.66
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-shuffle-66_20250329_103749/med_s1k_formatted",
          "timestamp": "2025-03-29T10:37:52.965456",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-66_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-shuffle-66",
          "timestamp": "2025-03-29T12:42:34Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-66/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-66/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:16:33.612785",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-66_eval.json"
        }
      }
    },
    "medqa-1k-random-shuffle-100": {
      "description": "1k random sample with 100% shuffle steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "shuffle",
            "rate": 1.0
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-shuffle-100_20250329_105309/med_s1k_formatted",
          "timestamp": "2025-03-29T10:53:12.859020",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-100_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-shuffle-100",
          "timestamp": "2025-03-29T12:58:13Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-100/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-100/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:15:56.968734",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-100_eval.json"
        }
      }
    },
    "medqa-1k-random-irrelevant-33": {
      "description": "1k random sample with 33% add irrelevant steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "add_irrelevant",
            "rate": 0.33
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-irrelevant-33_20250329_103749/med_s1k_formatted",
          "timestamp": "2025-03-29T10:38:02.889691",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-33_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-irrelevant-33",
          "timestamp": "2025-03-29T13:15:36Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-33/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-33/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:21:08.456701",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-33_eval.json"
        }
      }
    },
    "medqa-1k-random-irrelevant-66": {
      "description": "1k random sample with 66% add irrelevant steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "add_irrelevant",
            "rate": 0.66
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-irrelevant-66_20250329_103750/med_s1k_formatted",
          "timestamp": "2025-03-29T10:38:00.376226",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-66_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-irrelevant-66",
          "timestamp": "2025-03-29T13:34:44Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-66/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-66/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:34:47.179099",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-66_eval.json"
        }
      }
    },
    "medqa-1k-random-irrelevant-100": {
      "description": "1k random sample with 100% add irrelevant steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "add_irrelevant",
            "rate": 1.0
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-irrelevant-100_20250329_103750/med_s1k_formatted",
          "timestamp": "2025-03-29T10:38:00.402399",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-100_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-irrelevant-100",
          "timestamp": "2025-03-29T13:58:44Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-100/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-100/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:40:43.009995",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-100_eval.json"
        }
      }
    },
    "medqa-1k-random-wrong-answer-33": {
      "description": "1k random sample with 33% wrong answer perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "answer",
            "rate": 0.33
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-wrong-answer-33_20250329_103750/med_s1k_formatted",
          "timestamp": "2025-03-29T10:38:04.042223",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-wrong-answer-33_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-wrong-answer-33",
          "timestamp": "2025-03-29T14:13:26Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-wrong-answer-33/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-wrong-answer-33/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:30:38.471130",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-wrong-answer-33_eval.json"
        }
      }
    },
    "medqa-1k-random-wrong-answer-66": {
      "description": "1k random sample with 66% wrong answer perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "answer",
            "rate": 0.66
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-wrong-answer-66_20250329_103750/med_s1k_formatted",
          "timestamp": "2025-03-29T10:38:11.969368",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-wrong-answer-66_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-wrong-answer-66",
          "timestamp": "2025-03-29T14:28:19Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-wrong-answer-66/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-wrong-answer-66/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:34:22.576229",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-wrong-answer-66_eval.json"
        }
      }
    },
    "medqa-1k-random-decision-tree-extract": {
      "description": "1k uniform random sample with decision tree extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "decision-tree"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-decision-tree-extract_20250329_021551/med_s1k_formatted",
          "timestamp": "2025-03-29T02:16:39.316197",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-decision-tree-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-decision-tree-extract",
          "timestamp": "2025-03-29T06:10:32Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-decision-tree-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-decision-tree-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:54:04.193307",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-decision-tree-extract_eval.json"
        }
      }
    },
    "medqa-1k-random-qa-extract": {
      "description": "1k uniform random sample with Q&A extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "qa"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-qa-extract_20250329_021551/med_s1k_formatted",
          "timestamp": "2025-03-29T02:16:41.420492",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-qa-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-qa-extract",
          "timestamp": "2025-03-29T05:41:49Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-qa-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-qa-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:47:07.663123",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-qa-extract_eval.json"
        }
      }
    },
    "medqa-1k-random-socratic-extract": {
      "description": "1k uniform random sample with Socratic dialogue extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "socratic"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-socratic-extract_20250329_021551/med_s1k_formatted",
          "timestamp": "2025-03-29T02:16:48.700795",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-socratic-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-socratic-extract",
          "timestamp": "2025-03-29T05:57:05Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-socratic-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-socratic-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T19:49:47.357761",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-socratic-extract_eval.json"
        }
      }
    },
    "medqa-1k-random-wrong-answer-100": {
      "description": "1k random sample with 100% wrong answer perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "answer",
            "rate": 1.0
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-wrong-answer-100_20250329_103749/med_s1k_formatted",
          "timestamp": "2025-03-29T10:39:05.433347",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-wrong-answer-100_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-wrong-answer-100",
          "timestamp": "2025-03-29T14:43:43Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-wrong-answer-100/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-wrong-answer-100/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:45:52.057739",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-wrong-answer-100_eval.json"
        }
      }
    },
    "medqa-1k-random-collapse-33-restore": {
      "description": "1k random sample with 33% collapse consecutive steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "collapse_consecutive",
            "rate": 0.33
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-collapse-33-restore_20250329_102633/med_s1k_formatted",
          "timestamp": "2025-03-29T10:28:46.823111",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-33-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-collapse-33-restore",
          "timestamp": "2025-03-29T15:00:58Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-33-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-33-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:51:31.489003",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-33-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-collapse-66-restore": {
      "description": "1k random sample with 66% collapse consecutive steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "collapse_consecutive",
            "rate": 0.66
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-collapse-66-restore_20250329_102633/med_s1k_formatted",
          "timestamp": "2025-03-29T10:29:33.557850",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-66-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-collapse-66-restore",
          "timestamp": "2025-03-29T15:18:01Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-66-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-66-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:51:12.407473",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-66-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-collapse-100-restore": {
      "description": "1k random sample with 100% collapse consecutive steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "collapse_consecutive",
            "rate": 1.0
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-collapse-100-restore_20250329_102633/med_s1k_formatted",
          "timestamp": "2025-03-29T10:30:16.805901",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-100-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-collapse-100-restore",
          "timestamp": "2025-03-29T15:34:50Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-100-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-collapse-100-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T20:54:21.641498",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-collapse-100-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-skip-33-restore": {
      "description": "1k random sample with 33% skip steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "skip",
            "rate": 0.33
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-skip-33-restore_20250329_103749/med_s1k_formatted",
          "timestamp": "2025-03-29T10:39:10.446304",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-33-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-skip-33-restore",
          "timestamp": "2025-03-29T15:52:13Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-33-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-33-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T21:03:42.457796",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-33-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-skip-66-restore": {
      "description": "1k random sample with 66% skip steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "skip",
            "rate": 0.66
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-skip-66-restore_20250329_103749/med_s1k_formatted",
          "timestamp": "2025-03-29T10:38:58.724759",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-66-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-skip-66-restore",
          "timestamp": "2025-03-29T16:09:11Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-66-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-66-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T21:07:14.466359",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-66-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-skip-100-restore": {
      "description": "1k random sample with 100% skip steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "skip",
            "rate": 1.0
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-skip-100-restore_20250329_103749/med_s1k_formatted",
          "timestamp": "2025-03-29T10:38:49.201317",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-100-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-skip-100-restore",
          "timestamp": "2025-03-29T16:25:53Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-100-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-skip-100-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T21:05:56.610970",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-skip-100-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-shuffle-33-restore": {
      "description": "1k random sample with 33% shuffle steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "shuffle",
            "rate": 0.33
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-shuffle-33-restore_20250329_103749/med_s1k_formatted",
          "timestamp": "2025-03-29T10:39:07.022556",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-33-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-shuffle-33-restore",
          "timestamp": "2025-03-29T16:43:49Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-33-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-33-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T21:13:09.077454",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-33-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-shuffle-66-restore": {
      "description": "1k random sample with 66% shuffle steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "shuffle",
            "rate": 0.66
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-shuffle-66-restore_20250329_103749/med_s1k_formatted",
          "timestamp": "2025-03-29T10:39:01.423377",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-66-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-shuffle-66-restore",
          "timestamp": "2025-03-29T17:01:02Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-66-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-66-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T21:22:02.541539",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-66-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-shuffle-100-restore": {
      "description": "1k random sample with 100% shuffle steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "shuffle",
            "rate": 1.0
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-shuffle-100-restore_20250329_103749/med_s1k_formatted",
          "timestamp": "2025-03-29T10:39:00.349231",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-100-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-shuffle-100-restore",
          "timestamp": "2025-03-29T17:17:30Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-100-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-shuffle-100-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T21:23:53.643851",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-shuffle-100-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-irrelevant-33-restore": {
      "description": "1k random sample with 33% add irrelevant steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "add_irrelevant",
            "rate": 0.33
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-irrelevant-33-restore_20250329_103749/med_s1k_formatted",
          "timestamp": "2025-03-29T10:39:16.872397",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-33-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-irrelevant-33-restore",
          "timestamp": "2025-03-29T17:35:21Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-33-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-33-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T21:28:05.645486",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-33-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-irrelevant-66-restore": {
      "description": "1k random sample with 66% add irrelevant steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "add_irrelevant",
            "rate": 0.66
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-irrelevant-66-restore_20250329_102712/med_s1k_formatted",
          "timestamp": "2025-03-29T10:28:46.112100",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-66-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-irrelevant-66-restore",
          "timestamp": "2025-03-29T18:47:40Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-66-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-66-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T21:34:11.331002",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-66-restore_eval.json"
        }
      }
    },
    "medqa-1k-random-irrelevant-100-restore": {
      "description": "1k random sample with 100% add irrelevant steps perturbation and restoration",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "add_irrelevant",
            "rate": 1.0
          },
          "restore": true
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-irrelevant-100-restore_20250329_102712/med_s1k_formatted",
          "timestamp": "2025-03-29T10:29:09.928329",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-100-restore_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-irrelevant-100-restore",
          "timestamp": "2025-03-29T19:08:58Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-100-restore/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-irrelevant-100-restore/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-29T21:43:47.912423",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-irrelevant-100-restore_eval.json"
        }
      }
    },
    "s1-1k-random-no-cot": {
      "description": "1k random samples from the raw s1 dataset post-dropping NAs (54k examples) without CoT",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "none"
        },
        "datasets": {
          "curate": "s1-gemini-raw",
          "eval": "gpqa-diamond"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 50,
          "weight_decay": 0.1,
          "warmup_ratio": 0.1,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/mwornow/hf_cache/med-s1k/s1-1k-random-no-cot_20250330_221818/med_s1k_formatted",
          "timestamp": "2025-03-30T22:18:22.860185",
          "stats_file": "/share/pi/nigam/mwornow/med-s1/results/s1-1k-random-no-cot_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/mwornow/hf_cache/ckpts/s1-1k-random-no-cot",
          "timestamp": "2025-03-31T00:10:00Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/mwornow/hf_cache/eval/s1-1k-random-no-cot/best_modelgpqa-diamond.json",
          "metrics_path": "/share/pi/nigam/mwornow/hf_cache/eval/s1-1k-random-no-cot/best_modelgpqa-diamond_metrics.json",
          "timestamp": "2025-03-31T02:19:08.561172",
          "summary_file": "/share/pi/nigam/mwornow/med-s1/results/s1-1k-random-no-cot_eval.json"
        }
      }
    }
  }
}
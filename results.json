{
  "experiments": {
    "huatuo-25k": {
      "description": "Current configuration with existing hyperparameters",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "all",
          "huatuo_format": true,
          "n_samples": 25371
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 3,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/huatuo-25k_20250304_173206/med_s1k_formatted",
          "timestamp": "2025-03-04T17:32:13.385452",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/huatuo-25k_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/huatuo-25k",
          "timestamp": "2025-03-07T21:38:40Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo-25k/huatuo-25keval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo-25k/huatuo-25keval_data_metrics.json",
          "timestamp": "2025-03-07T22:55:15.075314",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/huatuo-25k_eval.json"
        }
      }
    },
    "huatuo-1k-random": {
      "description": "1k uniform random sample",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000
        },
        "training_params": {
          "learning_rate": 2e-06,
          "batch_size": 2,
          "num_epochs": 10,
          "weight_decay": 0.1,
          "warmup_ratio": 0.15,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/huatuo-1k-random_20250310_172443/med_s1k_formatted",
          "timestamp": "2025-03-10T17:24:45.149596",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/huatuo-1k-random_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/huatuo-1k-random",
          "timestamp": "2025-03-13T15:28:47Z",
          "metrics": null
        }
      }
    },
    "huatuo-5k-random": {
      "description": "5k uniform random sample",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 5000
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 8,
          "weight_decay": 0.1,
          "warmup_ratio": 0.1,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/huatuo-5k-random_20250310_172443/med_s1k_formatted",
          "timestamp": "2025-03-10T17:24:47.086996",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/huatuo-5k-random_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/huatuo-5k-random",
          "timestamp": "2025-03-13T04:40:24Z",
          "metrics": null
        }
      }
    },
    "huatuo-100-random": {
      "description": "100 uniform random sample; many epochs, very small LR, higher warmup ratio to avoid overshooting quickly",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 100
        },
        "training_params": {
          "learning_rate": 2e-06,
          "batch_size": 2,
          "num_epochs": 25,
          "weight_decay": 0.1,
          "warmup_ratio": 0.2,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/huatuo-100-random_20250313_023433/med_s1k_formatted",
          "timestamp": "2025-03-13T02:34:42.074516",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/huatuo-100-random_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/huatuo-100-random",
          "timestamp": "2025-03-13T11:49:45Z",
          "metrics": null
        }
      }
    },
    "huatuo-1k-embedding-similarity-question": {
      "description": "1k sample using embedding similarity on questions",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "embedding-similarity",
          "huatuo_format": true,
          "n_samples": 1000,
          "column": "Question"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 15,
          "weight_decay": 0.1,
          "warmup_ratio": 0.1,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {}
    },
    "huatuo-1k-embedding-similarity-cot": {
      "description": "1k sample using embedding similarity on complex cot",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "embedding-similarity",
          "huatuo_format": true,
          "n_samples": 1000,
          "column": "Complex_CoT"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 15,
          "weight_decay": 0.1,
          "warmup_ratio": 0.1,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {}
    },
    "huatuo-1k-embedding-diversity-question-cluster-10-outlier-5": {
      "description": "1k sample using embedding diversity on questions with 10% clusters and 5% outliers",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "embedding-diversity",
          "huatuo_format": true,
          "n_samples": 1000,
          "column": "Question",
          "cluster_percentage": 10,
          "outlier_percentage": 5
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 15,
          "weight_decay": 0.1,
          "warmup_ratio": 0.1,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {}
    },
    "huatuo-1k-embedding-diversity-cot-cluster-10-outlier-5": {
      "description": "1k sample using embedding diversity on complex cot with 10% clusters and 5% outliers",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "embedding-diversity",
          "huatuo_format": true,
          "n_samples": 1000,
          "column": "Complex_CoT",
          "cluster_percentage": 10,
          "outlier_percentage": 5
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 15,
          "weight_decay": 0.1,
          "warmup_ratio": 0.1,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {}
    },
    "huatuo-1k-difficulty-substring": {
      "description": "1k sample using difficulty substring filtering",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "difficulty-substring",
          "huatuo_format": true,
          "n_samples": 1000
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 15,
          "weight_decay": 0.1,
          "warmup_ratio": 0.1,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {}
    },
    "huatuo-1k-novelty-answer": {
      "description": "1k sample using novelty answer filtering",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "novelty-answer",
          "huatuo_format": true,
          "n_samples": 1000
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 15,
          "weight_decay": 0.1,
          "warmup_ratio": 0.1,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {}
    }
  }
}
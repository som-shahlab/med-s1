{
  "experiments": {
    "base": {
      "description": "Base LLaMA model without fine-tuning",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": null,
        "training_params": null,
        "datasets": {
          "curate": null,
          "eval": "huatuo-eval"
        }
      },
      "results": {
        "curation": null,
        "training": null,
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base/Llama-3.1-8B-Instructeval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base/Llama-3.1-8B-Instructeval_data_metrics.json",
          "timestamp": "2025-03-15T22:16:49.150561",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/base_eval.json"
        }
      }
    },
    "base-step-prompt": {
      "description": "Base model with step-by-step prompting",
      "config": {
        "model_key": "llama3.1:8b",
        "prompting": "step",
        "datasets": {
          "curate": "huatuo-sft",
          "eval": "huatuo-eval"
        }
      },
      "results": {
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base-step-prompt/Llama-3.1-8B-Instructeval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base-step-prompt/Llama-3.1-8B-Instructeval_data_metrics.json",
          "timestamp": "2025-03-15T05:51:41.888151",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/base-step-prompt_eval.json"
        }
      }
    },
    "huatuo": {
      "description": "Pre-trained HuatuoGPT medical model",
      "config": {
        "model_key": "huatuo:8b",
        "curation": null,
        "training_params": null,
        "datasets": "same as base"
      },
      "results": {
        "curation": null,
        "training": null,
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo/HuatuoGPT-o1-8Beval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo/HuatuoGPT-o1-8Beval_data_metrics.json",
          "timestamp": "2025-03-15T22:29:05.214214",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/huatuo_eval.json"
        }
      }
    },
    "medqa-1k-random": {
      "description": "1k uniform random sample",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 10,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random_20250315_174440/med_s1k_formatted",
          "timestamp": "2025-03-15T17:44:42.688674",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random",
          "timestamp": "2025-03-15T20:31:04Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-15T22:43:39.265496",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random_eval.json"
        }
      }
    },
    "medqa-1k-embedding-diversity-question-cluster-10-outlier-5": {
      "description": "1k sample using embedding diversity on questions with 10% clusters and 5% outliers",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "embedding-diversity",
          "huatuo_format": true,
          "n_samples": 1000,
          "column": "Question",
          "cluster_percentage": 10,
          "outlier_percentage": 5
        },
        "training_params": "same as medqa-1k-random",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-embedding-diversity-question-cluster-10-outlier-5_20250315_174140/med_s1k_formatted",
          "timestamp": "2025-03-15T17:41:42.398295",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-embedding-diversity-question-cluster-10-outlier-5_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-embedding-diversity-question-cluster-10-outlier-5",
          "timestamp": "2025-03-15T20:40:12Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-embedding-diversity-question-cluster-10-outlier-5/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-embedding-diversity-question-cluster-10-outlier-5/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-15T22:57:04.108404",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-embedding-diversity-question-cluster-10-outlier-5_eval.json"
        }
      }
    },
    "medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5": {
      "description": "1k sample using embedding diversity on questions with 10% clusters and 5% outliers, ranked by CoT length",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "embedding-diversity",
          "huatuo_format": true,
          "n_samples": 1000,
          "column": "Question",
          "cluster_percentage": 10,
          "outlier_percentage": 5,
          "intra_group_ranking": "cot-length"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 8,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5_20250315_174300/med_s1k_formatted",
          "timestamp": "2025-03-15T17:43:02.524140",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
          "timestamp": "2025-03-15T20:52:17Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-15T23:05:18.243880",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5_eval.json"
        }
      }
    },
    "medqa-25k": {
      "description": "Full dataset with all examples",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "all",
          "huatuo_format": true,
          "n_samples": 25371
        },
        "training_params": {
          "learning_rate": 3e-06,
          "batch_size": 2,
          "num_epochs": 3,
          "weight_decay": 0.1,
          "warmup_ratio": 0.02,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-25k_20250315_174228/med_s1k_formatted",
          "timestamp": "2025-03-15T17:42:34.955957",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-25k_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-25k",
          "timestamp": "2025-03-18T19:20:35Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-25k/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-25k/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-20T02:56:57.087364",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-25k_eval.json"
        }
      }
    },
    "medqa-1k-random-step-extract": {
      "description": "1k uniform random sample with step extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-step-extract_20250327_232841/med_s1k_formatted",
          "timestamp": "2025-03-28T00:18:51.109687",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-step-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-step-extract",
          "timestamp": "2025-03-28T14:48:03Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-step-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-step-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-15T23:04:27.582677",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-step-extract_eval.json"
        }
      }
    },
    "medqa-1k-random-no-cot": {
      "description": "1k uniform random sample without CoT",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "none"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 15,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-no-cot_20250315_174325/med_s1k_formatted",
          "timestamp": "2025-03-15T17:43:27.257948",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-no-cot_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-no-cot",
          "timestamp": "2025-03-15T23:51:24Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-no-cot/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-no-cot/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-15T23:56:47.782596",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-no-cot_eval.json"
        }
      }
    },
    "medqa-1k-random-1-sentence-extract": {
      "description": "1k uniform random sample with 1-sentence CoT extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "1-sentence"
        },
        "training_params": {
          "learning_rate": 1e-05,
          "batch_size": 2,
          "num_epochs": 12,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-1-sentence-extract_20250315_191432/med_s1k_formatted",
          "timestamp": "2025-03-15T19:16:27.998188",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-1-sentence-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-1-sentence-extract",
          "timestamp": "2025-03-18T17:59:26Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-1-sentence-extract/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-1k-random-1-sentence-extract/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-15T22:53:23.551424",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-1-sentence-extract_eval.json"
        }
      }
    },
    "medqa-5k-random-no-cot": {
      "description": "5k uniform random sample without CoT",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 5000,
          "extract": "none"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 6,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-5k-random-no-cot_20250315_174413/med_s1k_formatted",
          "timestamp": "2025-03-15T17:44:16.109350",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-5k-random-no-cot_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-5k-random-no-cot",
          "timestamp": "2025-03-18T18:13:55Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-5k-random-no-cot/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-5k-random-no-cot/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-20T02:48:45.125184",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-5k-random-no-cot_eval.json"
        }
      }
    },
    "medqa-10k-random-no-cot": {
      "description": "10k uniform random sample without CoT",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 10000,
          "extract": "none"
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 4,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        },
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-10k-random-no-cot_20250315_173858/med_s1k_formatted",
          "timestamp": "2025-03-15T17:39:01.587844",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-10k-random-no-cot_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-10k-random-no-cot",
          "timestamp": "2025-03-18T18:28:42Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-10k-random-no-cot/best_modeleval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/medqa-10k-random-no-cot/best_modeleval_data_metrics.json",
          "timestamp": "2025-03-20T02:48:55.758592",
          "summary_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-10k-random-no-cot_eval.json"
        }
      }
    },
    "medqa-1k-random-note-extract": {
      "description": "1k uniform random sample with auto-determined clinical note format extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "note"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-note-extract_20250328_003819/med_s1k_formatted",
          "timestamp": "2025-03-28T01:55:01.385586",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-note-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-note-extract",
          "timestamp": "2025-03-28T15:48:03Z",
          "metrics": null
        },
        "eval": null
      }
    },
    "medqa-1k-random-list-extract": {
      "description": "1k uniform random sample with bullet-point list extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "list"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-list-extract_20250328_001916/med_s1k_formatted",
          "timestamp": "2025-03-28T00:37:58.484094",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-list-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-list-extract",
          "timestamp": "2025-03-28T15:31:32Z",
          "metrics": null
        },
        "eval": null
      }
    },
    "medqa-1k-random-markdown-extract": {
      "description": "1k uniform random sample with markdown document extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "markdown"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-markdown-extract_20250327_231721/med_s1k_formatted",
          "timestamp": "2025-03-27T23:20:50.069471",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-markdown-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-markdown-extract",
          "timestamp": "2025-03-28T15:18:42Z",
          "metrics": null
        },
        "eval": null
      }
    },
    "medqa-1k-random-evidence-extract": {
      "description": "1k uniform random sample with auto-determined clinical note format extraction",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step-evidence"
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/medqa-1k-random-evidence-extract_20250327_232935/med_s1k_formatted",
          "timestamp": "2025-03-28T01:02:53.354538",
          "stats_file": "/share/pi/nigam/users/calebwin/med-s1/results/medqa-1k-random-evidence-extract_curation.json"
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/medqa-1k-random-evidence-extract",
          "timestamp": "2025-03-28T15:04:10Z",
          "metrics": null
        },
        "eval": null
      }
    },
    "medqa-1k-random-collapse-33": {
      "description": "1k random sample with 33% collapse consecutive steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "collapse_consecutive",
            "rate": 0.33
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-collapse-66": {
      "description": "1k random sample with 66% collapse consecutive steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "collapse_consecutive",
            "rate": 0.66
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-collapse-100": {
      "description": "1k random sample with 100% collapse consecutive steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "collapse_consecutive",
            "rate": 1.0
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-skip-33": {
      "description": "1k random sample with 33% skip steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "skip",
            "rate": 0.33
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-skip-66": {
      "description": "1k random sample with 66% skip steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "skip",
            "rate": 0.66
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-skip-100": {
      "description": "1k random sample with 100% skip steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "skip",
            "rate": 1.0
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-shuffle-33": {
      "description": "1k random sample with 33% shuffle steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "shuffle",
            "rate": 0.33
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-shuffle-66": {
      "description": "1k random sample with 66% shuffle steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "shuffle",
            "rate": 0.66
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-shuffle-100": {
      "description": "1k random sample with 100% shuffle steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "shuffle",
            "rate": 1.0
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-irrelevant-33": {
      "description": "1k random sample with 33% add irrelevant steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "add_irrelevant",
            "rate": 0.33
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-irrelevant-66": {
      "description": "1k random sample with 66% add irrelevant steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "add_irrelevant",
            "rate": 0.66
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-irrelevant-100": {
      "description": "1k random sample with 100% add irrelevant steps perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "add_irrelevant",
            "rate": 1.0
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    },
    "medqa-1k-random-wrong-answer": {
      "description": "1k random sample with wrong answer perturbation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "huatuo_format": true,
          "n_samples": 1000,
          "extract": "step",
          "perturbation": {
            "type": "answer"
          }
        },
        "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
        "datasets": "same as base-step-prompt"
      }
    }
  },
  "medqa-1k-random-collapse-33-restore": {
    "description": "1k random sample with 33% collapse consecutive steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "collapse_consecutive",
          "rate": 0.33
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-collapse-66-restore": {
    "description": "1k random sample with 66% collapse consecutive steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "collapse_consecutive",
          "rate": 0.66
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-collapse-100-restore": {
    "description": "1k random sample with 100% collapse consecutive steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "collapse_consecutive",
          "rate": 1.0
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-skip-33-restore": {
    "description": "1k random sample with 33% skip steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "skip",
          "rate": 0.33
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-skip-66-restore": {
    "description": "1k random sample with 66% skip steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "skip",
          "rate": 0.66
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-skip-100-restore": {
    "description": "1k random sample with 100% skip steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "skip",
          "rate": 1.0
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-shuffle-33-restore": {
    "description": "1k random sample with 33% shuffle steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "shuffle",
          "rate": 0.33
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-shuffle-66-restore": {
    "description": "1k random sample with 66% shuffle steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "shuffle",
          "rate": 0.66
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-shuffle-100-restore": {
    "description": "1k random sample with 100% shuffle steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "shuffle",
          "rate": 1.0
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-irrelevant-33-restore": {
    "description": "1k random sample with 33% add irrelevant steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "add_irrelevant",
          "rate": 0.33
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-irrelevant-66-restore": {
    "description": "1k random sample with 66% add irrelevant steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "add_irrelevant",
          "rate": 0.66
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  },
  "medqa-1k-random-irrelevant-100-restore": {
    "description": "1k random sample with 100% add irrelevant steps perturbation and restoration",
    "config": {
      "model_key": "llama3.1:8b",
      "curation": {
        "method": "random",
        "huatuo_format": true,
        "n_samples": 1000,
        "extract": "step",
        "perturbation": {
          "type": "add_irrelevant",
          "rate": 1.0
        },
        "restore": true
      },
      "training_params": "same as medqa-1k-embedding-difficulty-diversity-question-cluster-10-outlier-5",
      "datasets": "same as base-step-prompt"
    }
  }
}
{
  "experiments": {
    "huatuo-25k": {
      "description": "Current configuration with existing hyperparameters",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "all",
          "huatuo_format": true,
          "n_samples": 25371
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 2,
          "num_epochs": 3,
          "weight_decay": 0.1,
          "warmup_ratio": 0.05,
          "optimizer": {
            "no_decay_params": [
              "bias",
              "LayerNorm.weight"
            ],
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "adam_epsilon": 1e-08
          },
          "gradient_accumulation_steps": 16
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/huatuo-25k_20250304_173206/med_s1k_formatted",
          "timestamp": "2025-03-04T17:32:13.385452",
          "stats": {
            "total_examples": 25371,
            "selected_examples": 25371,
            "filtered_examples": 24371,
            "filter_reasons": {
              "not_selected_in_sampling": 17407,
              "base_model_correct": 6964
            },
            "specialty_distribution": {
              "General Knowledge": 750,
              "Infectious Disease": 731,
              "Neurology": 712,
              "Gastroenterology": 688,
              "Math": 600,
              "Psychiatry": 597,
              "Obstetrics and Gynecology": 551,
              "Endocrinology, Diabetes and Metabolism": 542,
              "Dentistry": 501,
              "Pulmonary Disease": 474,
              "Physics": 444,
              "Public Health and General Preventive Medicine": 426,
              "Cardiovascular Disease": 422,
              "Nephrology": 415,
              "Maternal-Fetal Medicine": 406,
              "Ophthalmology": 391,
              "Hematology": 374,
              "Otolaryngology - Head and Neck Surgery": 369,
              "Health Care Administration, Leadership, and Management": 356,
              "Pediatrics": 293,
              "General Surgery": 289,
              "Dermatology": 271,
              "Rheumatology": 264,
              "Orthopaedic Surgery": 256,
              "Clinical Genetics and Genomics (MD)": 249,
              "Internal Medicine": 239,
              "Urology": 209,
              "Medical Toxicology": 204,
              "Anesthesiology": 202,
              "Economics": 196,
              "Pediatric Infectious Diseases": 183,
              "Allergy and Immunology": 178,
              "Pediatric Hematology-Oncology": 177,
              "Law": 176,
              "Medical Oncology": 175,
              "Emergency Medicine": 175,
              "Vascular Surgery": 162,
              "Neonatal-Perinatal Medicine": 160,
              "Pathology - Anatomic": 150,
              "Clinical Informatics": 147,
              "Medical Physics (Diagnostic, Nuclear, Therapeutic)": 145,
              "Computer Science": 143,
              "Clinical Biochemical Genetics": 133,
              "Pediatric Endocrinology": 133,
              "Chemistry": 131,
              "Pediatric Cardiology": 122,
              "Pathology - Forensic": 121,
              "Reproductive Endocrinology and Infertility": 114,
              "Pediatric Nephrology": 111,
              "Gynecologic Oncology": 109,
              "Pediatric Gastroenterology": 109,
              "Pathology - Chemical": 109,
              "Critical Care Medicine": 106,
              "Education": 102,
              "Neurology with Special Qualification in Child Neurology": 100,
              "Pediatric Surgery": 99,
              "Neurological Surgery": 94,
              "Pediatric Dermatology": 86,
              "Statistics": 79,
              "Clinical Cardiac Electrophysiology": 79,
              "Family Medicine": 74,
              "Forensic Psychiatry": 74,
              "Pediatric Pulmonology": 70,
              "Addiction Medicine": 68,
              "Developmental-Behavioral Pediatrics": 66,
              "Surgery of the Hand": 64,
              "Child and Adolescent Psychiatry": 58,
              "Occupational and Environmental Medicine": 58,
              "Colon and Rectal Surgery": 54,
              "Advanced Heart Failure and Transplant Cardiology": 54,
              "Pathology - Medical Microbiology": 52,
              "Thoracic and Cardiac Surgery": 51,
              "Sports Medicine": 51,
              "Vascular Neurology": 50,
              "Neurotology": 50,
              "Complex Pediatric Otolaryngology": 45,
              "History": 45,
              "Orthopaedic Sports Medicine": 42,
              "Diagnostic Radiology": 42,
              "Physical Medicine and Rehabilitation": 41,
              "Complex General Surgical Oncology": 39,
              "Sleep Medicine": 39,
              "Pediatric Emergency Medicine": 38,
              "Pediatric Urology": 36,
              "Surgical Critical Care": 33,
              "Neurocritical Care": 33,
              "Astronomy": 33,
              "Geriatric Medicine": 32,
              "Pediatric Critical Care Medicine": 32,
              "Nuclear Medicine": 31,
              "Urogynecology and Reconstructive Pelvic Surgery": 30,
              "Neuropathology": 29,
              "Transplant Hepatology": 27,
              "Aerospace Medicine": 27,
              "Plastic Surgery": 25,
              "Pediatric Anesthesiology": 25,
              "Politics": 24,
              "Pathology - Anatomic/Pathology - Clinical": 24,
              "Pain Medicine": 23,
              "Undersea and Hyperbaric Medicine": 22,
              "Adolescent Medicine": 22,
              "Pediatric Rheumatology": 21,
              "Hospice and Palliative Medicine": 21,
              "Molecular Genetic Pathology": 20,
              "Medical Biochemical Genetics": 20,
              "Radiation Oncology": 18,
              "Interventional Cardiology": 18,
              "Hematopathology": 17,
              "Dermatopathology": 17,
              "Pathology - Pediatric": 14,
              "Complex Family Planning": 14,
              "Adult Cardiac Anesthesiology": 14,
              "Laboratory Genetics and Genomics": 14,
              "Neuroradiology": 12,
              "Adult Congenital Heart Disease": 12,
              "Blood Banking/Transfusion Medicine": 12,
              "Pathology - Clinical": 10,
              "Podiatry": 10,
              "Child Abuse Pediatrics": 9,
              "Plastic Surgery Within the Head and Neck*": 6,
              "Cytopathology": 6,
              "Spinal Cord Injury Medicine": 6,
              "Neuromuscular Medicine": 5,
              "Internal Medicine-Critical Care Medicine": 5,
              "Geography": 5,
              "Religion": 4,
              "Pediatric Radiology": 4,
              "Interventional Radiology and Diagnostic Radiology": 4,
              "Clinical Neurophysiology": 4,
              "Geriatric Psychiatry": 4,
              "Addiction Psychiatry": 3,
              "Emergency Medical Services": 3,
              "Consultation-Liaison Psychiatry": 3,
              "Optometry": 3,
              "Epilepsy": 2,
              "Pediatric Rehabilitation Medicine": 2,
              "Pathology - Molecular Genetic": 2,
              "Neurodevelopmental Disabilities": 2,
              "Nuclear Radiology": 1,
              "Biology": 1,
              "Congenital Cardiac Surgery": 1
            }
          }
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/huatuo-25k",
          "timestamp": "2025-03-07T18:49:23Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-1k/med-s1-1keval_data_strict-prompt.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-1k/med-s1-1keval_data_strict-prompt_metrics.json",
          "timestamp": "2025-02-26T18:42:38.091401",
          "summary": {
            "MedMCQA_validation": {
              "accuracy": 0.4111881424814726,
              "accuracy_ci": [
                0.39444776476213245,
                0.42457566339947406
              ],
              "num_correct": 1715,
              "total_examples": 4183,
              "num_answered": 1720
            },
            "MedQA_USLME_test": {
              "accuracy": 0.4783974862529458,
              "accuracy_ci": [
                0.44776119402985076,
                0.5003927729772192
              ],
              "num_correct": 604,
              "total_examples": 1273,
              "num_answered": 609
            },
            "PubMedQA_test": {
              "accuracy": 0.645,
              "accuracy_ci": [
                0.615,
                0.673025
              ],
              "num_correct": 645,
              "total_examples": 1000,
              "num_answered": 643
            },
            "MMLU-Pro_Medical_test": {
              "accuracy": 0.3758957654723127,
              "accuracy_ci": [
                0.34592833876221496,
                0.3935016286644951
              ],
              "num_correct": 564,
              "total_examples": 1535,
              "num_answered": 577
            },
            "GPQA_Medical_test": {
              "accuracy": 0.35324675324675325,
              "accuracy_ci": [
                0.2935064935064935,
                0.38701298701298703
              ],
              "num_correct": 131,
              "total_examples": 385,
              "num_answered": 136
            }
          }
        }
      }
    },
    "med-s1-1k-tuned": {
      "description": "Tuned version with adjusted hyperparameters",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "s1",
          "n_samples": 1000
        },
        "training_params": {
          "learning_rate": 2e-06,
          "batch_size": 16,
          "num_epochs": 5,
          "weight_decay": 0.001
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/plumbing_test_001_20250219_145607/med_s1k_formatted",
          "timestamp": null
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/med-s1-1k-tuned",
          "timestamp": "2025-02-27T18:43:27Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-1k-tuned/Llama-3.1-8B-Instructeval_data_strict-prompt.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-1k-tuned/Llama-3.1-8B-Instructeval_data_strict-prompt_metrics.json",
          "timestamp": "2025-02-27T21:20:22.586668",
          "summary": {
            "MedMCQA_validation": {
              "accuracy": 0.5558211809705953,
              "accuracy_ci": [
                0.5397979918718623,
                0.5706430791298112
              ],
              "num_correct": 2325,
              "total_examples": 4183,
              "num_answered": 2324
            },
            "MedQA_USLME_test": {
              "accuracy": 0.6252945797329144,
              "accuracy_ci": [
                0.6001374705420268,
                0.6504320502749411
              ],
              "num_correct": 796,
              "total_examples": 1273,
              "num_answered": 794
            },
            "PubMedQA_test": {
              "accuracy": 0.751,
              "accuracy_ci": [
                0.724,
                0.779
              ],
              "num_correct": 751,
              "total_examples": 1000,
              "num_answered": 751
            },
            "MMLU-Pro_Medical_test": {
              "accuracy": 0.5889250814332248,
              "accuracy_ci": [
                0.5661074918566775,
                0.6143322475570032
              ],
              "num_correct": 904,
              "total_examples": 1535,
              "num_answered": 904
            },
            "GPQA_Medical_test": {
              "accuracy": 0.41794871794871796,
              "accuracy_ci": [
                0.3717948717948718,
                0.46923076923076923
              ],
              "num_correct": 163,
              "total_examples": 390,
              "num_answered": 163
            }
          }
        }
      }
    },
    "med-s1-1k-curated": {
      "description": "Iteration on curation methodology",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "s1",
          "n_samples": 1000,
          "specialty_weights": {
            "Internal Medicine": 1.5,
            "Emergency Medicine": 1.5,
            "Critical Care": 1.2
          }
        },
        "training_params": {
          "learning_rate": 2e-05,
          "batch_size": 8,
          "num_epochs": 8
        }
      },
      "results": {
        "curation": {
          "dataset_path": null,
          "timestamp": null
        },
        "training": {
          "model_path": null,
          "timestamp": null,
          "metrics": null
        },
        "eval": {
          "results_path": null,
          "timestamp": null,
          "metrics": null
        }
      }
    },
    "med-s1-5k": {
      "description": "Larger curated dataset with 5k samples",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "s1",
          "n_samples": 5000
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 32,
          "num_epochs": 3,
          "weight_decay": 0.001
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/med-s1-5k_20250227_140603/med_s1k_formatted",
          "timestamp": "2025-02-27T14:06:05.330212",
          "stats": {
            "total_examples": 25371,
            "selected_examples": 1000,
            "filtered_examples": 24371,
            "filter_reasons": {
              "not_selected_in_sampling": 17407,
              "base_model_correct": 6964
            },
            "specialty_distribution": {
              "Medical Physics (Diagnostic, Nuclear, Therapeutic)": 13,
              "Clinical Biochemical Genetics": 13,
              "Physics": 13,
              "Pain Medicine": 12,
              "Pulmonary Disease": 12,
              "Pediatric Critical Care Medicine": 12,
              "Neurotology": 12,
              "Statistics": 12,
              "Obstetrics and Gynecology": 12,
              "Adult Congenital Heart Disease": 12,
              "Hospice and Palliative Medicine": 12,
              "Forensic Psychiatry": 11,
              "Orthopaedic Surgery": 11,
              "Emergency Medicine": 11,
              "Pediatrics": 11,
              "Maternal-Fetal Medicine": 11,
              "Adult Cardiac Anesthesiology": 11,
              "General Knowledge": 11,
              "Clinical Genetics and Genomics (MD)": 11,
              "Neonatal-Perinatal Medicine": 11,
              "Aerospace Medicine": 11,
              "Nephrology": 10,
              "Pathology - Anatomic": 10,
              "Complex Pediatric Otolaryngology": 10,
              "Pediatric Gastroenterology": 10,
              "Health Care Administration, Leadership, and Management": 10,
              "Infectious Disease": 10,
              "Medical Toxicology": 10,
              "Laboratory Genetics and Genomics": 10,
              "Endocrinology, Diabetes and Metabolism": 10,
              "Pathology - Clinical": 10,
              "General Surgery": 9,
              "Psychiatry": 9,
              "Pathology - Medical Microbiology": 9,
              "Occupational and Environmental Medicine": 9,
              "Pediatric Pulmonology": 9,
              "Pathology - Chemical": 9,
              "Public Health and General Preventive Medicine": 9,
              "Neurocritical Care": 9,
              "Dermatopathology": 9,
              "Molecular Genetic Pathology": 9,
              "Interventional Cardiology": 9,
              "Neurology with Special Qualification in Child Neurology": 9,
              "Child Abuse Pediatrics": 9,
              "Sleep Medicine": 9,
              "Computer Science": 8,
              "Ophthalmology": 8,
              "Gynecologic Oncology": 8,
              "Clinical Informatics": 8,
              "Addiction Medicine": 8,
              "Math": 8,
              "Cardiovascular Disease": 8,
              "Pediatric Endocrinology": 8,
              "Neuroradiology": 8,
              "Transplant Hepatology": 8,
              "Economics": 8,
              "Complex General Surgical Oncology": 8,
              "Surgical Critical Care": 8,
              "Reproductive Endocrinology and Infertility": 8,
              "Dermatology": 8,
              "Critical Care Medicine": 8,
              "Pediatric Infectious Diseases": 8,
              "Developmental-Behavioral Pediatrics": 8,
              "Geriatric Medicine": 7,
              "Physical Medicine and Rehabilitation": 7,
              "Medical Oncology": 7,
              "Diagnostic Radiology": 7,
              "Hematology": 7,
              "Pediatric Urology": 7,
              "Dentistry": 7,
              "Adolescent Medicine": 7,
              "Pediatric Hematology-Oncology": 7,
              "Pathology - Pediatric": 7,
              "Anesthesiology": 7,
              "Rheumatology": 7,
              "Pathology - Anatomic/Pathology - Clinical": 7,
              "Thoracic and Cardiac Surgery": 6,
              "Education": 6,
              "Plastic Surgery Within the Head and Neck*": 6,
              "Pathology - Forensic": 6,
              "Cytopathology": 6,
              "Pediatric Rheumatology": 6,
              "Advanced Heart Failure and Transplant Cardiology": 6,
              "Child and Adolescent Psychiatry": 6,
              "Neurology": 6,
              "Politics": 6,
              "Law": 6,
              "Radiation Oncology": 6,
              "Medical Biochemical Genetics": 6,
              "Blood Banking/Transfusion Medicine": 6,
              "Nuclear Medicine": 6,
              "Orthopaedic Sports Medicine": 6,
              "Allergy and Immunology": 6,
              "Clinical Cardiac Electrophysiology": 6,
              "Podiatry": 6,
              "Pediatric Cardiology": 6,
              "Family Medicine": 5,
              "Gastroenterology": 5,
              "Urology": 5,
              "Surgery of the Hand": 5,
              "Colon and Rectal Surgery": 5,
              "Undersea and Hyperbaric Medicine": 5,
              "Chemistry": 5,
              "Pediatric Emergency Medicine": 5,
              "History": 5,
              "Complex Family Planning": 5,
              "Neurological Surgery": 5,
              "Internal Medicine": 5,
              "Geography": 5,
              "Urogynecology and Reconstructive Pelvic Surgery": 5,
              "Neuromuscular Medicine": 5,
              "Otolaryngology - Head and Neck Surgery": 5,
              "Pediatric Dermatology": 5,
              "Internal Medicine-Critical Care Medicine": 5,
              "Vascular Neurology": 5,
              "Neuropathology": 4,
              "Plastic Surgery": 4,
              "Pediatric Radiology": 4,
              "Clinical Neurophysiology": 4,
              "Pediatric Surgery": 4,
              "Religion": 4,
              "Geriatric Psychiatry": 4,
              "Astronomy": 4,
              "Vascular Surgery": 4,
              "Interventional Radiology and Diagnostic Radiology": 4,
              "Pediatric Nephrology": 4,
              "Hematopathology": 3,
              "Pediatric Anesthesiology": 3,
              "Sports Medicine": 3,
              "Optometry": 3,
              "Consultation-Liaison Psychiatry": 3,
              "Spinal Cord Injury Medicine": 3,
              "Emergency Medical Services": 3,
              "Addiction Psychiatry": 3,
              "Pediatric Rehabilitation Medicine": 2,
              "Pathology - Molecular Genetic": 2,
              "Epilepsy": 2,
              "Neurodevelopmental Disabilities": 2,
              "Nuclear Radiology": 1,
              "Biology": 1,
              "Congenital Cardiac Surgery": 1
            }
          }
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/med-s1-5k",
          "timestamp": "2025-02-27T18:22:54Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-5k/med-s1-5keval_data_strict-prompt.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-5k/med-s1-5keval_data_strict-prompt_metrics.json",
          "timestamp": "2025-02-27T20:37:47.999052",
          "summary": {
            "MedMCQA_validation": {
              "accuracy": 0.5785321539564906,
              "accuracy_ci": [
                0.5625089648577576,
                0.5924037771934019
              ],
              "num_correct": 2417,
              "total_examples": 4183,
              "num_answered": 2420
            },
            "MedQA_USLME_test": {
              "accuracy": 0.6465043205027494,
              "accuracy_ci": [
                0.6221327572663001,
                0.6732325216025137
              ],
              "num_correct": 823,
              "total_examples": 1273,
              "num_answered": 821
            },
            "PubMedQA_test": {
              "accuracy": 0.763,
              "accuracy_ci": [
                0.736,
                0.790025
              ],
              "num_correct": 763,
              "total_examples": 1000,
              "num_answered": 763
            },
            "MMLU-Pro_Medical_test": {
              "accuracy": 0.5824104234527687,
              "accuracy_ci": [
                0.5595928338762215,
                0.6078338762214983
              ],
              "num_correct": 894,
              "total_examples": 1535,
              "num_answered": 886
            },
            "GPQA_Medical_test": {
              "accuracy": 0.4794871794871795,
              "accuracy_ci": [
                0.4307692307692308,
                0.5282692307692307
              ],
              "num_correct": 187,
              "total_examples": 390,
              "num_answered": 186
            }
          }
        }
      }
    },
    "med-s1-25k": {
      "description": "Full dataset without curation",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "all",
          "n_samples": 25371
        },
        "training_params": {
          "learning_rate": 1e-05,
          "batch_size": 64,
          "num_epochs": 2,
          "weight_decay": 0.001
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/med-s1-25k_20250227_140738/med_s1k_formatted",
          "timestamp": "2025-02-27T14:07:45.576891",
          "stats": {
            "total_examples": 25371,
            "selected_examples": 25371,
            "filtered_examples": 24371,
            "filter_reasons": {
              "not_selected_in_sampling": 17407,
              "base_model_correct": 6964
            },
            "specialty_distribution": {
              "General Knowledge": 750,
              "Infectious Disease": 731,
              "Neurology": 712,
              "Gastroenterology": 688,
              "Math": 600,
              "Psychiatry": 597,
              "Obstetrics and Gynecology": 551,
              "Endocrinology, Diabetes and Metabolism": 542,
              "Dentistry": 501,
              "Pulmonary Disease": 474,
              "Physics": 444,
              "Public Health and General Preventive Medicine": 426,
              "Cardiovascular Disease": 422,
              "Nephrology": 415,
              "Maternal-Fetal Medicine": 406,
              "Ophthalmology": 391,
              "Hematology": 374,
              "Otolaryngology - Head and Neck Surgery": 369,
              "Health Care Administration, Leadership, and Management": 356,
              "Pediatrics": 293,
              "General Surgery": 289,
              "Dermatology": 271,
              "Rheumatology": 264,
              "Orthopaedic Surgery": 256,
              "Clinical Genetics and Genomics (MD)": 249,
              "Internal Medicine": 239,
              "Urology": 209,
              "Medical Toxicology": 204,
              "Anesthesiology": 202,
              "Economics": 196,
              "Pediatric Infectious Diseases": 183,
              "Allergy and Immunology": 178,
              "Pediatric Hematology-Oncology": 177,
              "Law": 176,
              "Medical Oncology": 175,
              "Emergency Medicine": 175,
              "Vascular Surgery": 162,
              "Neonatal-Perinatal Medicine": 160,
              "Pathology - Anatomic": 150,
              "Clinical Informatics": 147,
              "Medical Physics (Diagnostic, Nuclear, Therapeutic)": 145,
              "Computer Science": 143,
              "Clinical Biochemical Genetics": 133,
              "Pediatric Endocrinology": 133,
              "Chemistry": 131,
              "Pediatric Cardiology": 122,
              "Pathology - Forensic": 121,
              "Reproductive Endocrinology and Infertility": 114,
              "Pediatric Nephrology": 111,
              "Gynecologic Oncology": 109,
              "Pediatric Gastroenterology": 109,
              "Pathology - Chemical": 109,
              "Critical Care Medicine": 106,
              "Education": 102,
              "Neurology with Special Qualification in Child Neurology": 100,
              "Pediatric Surgery": 99,
              "Neurological Surgery": 94,
              "Pediatric Dermatology": 86,
              "Statistics": 79,
              "Clinical Cardiac Electrophysiology": 79,
              "Family Medicine": 74,
              "Forensic Psychiatry": 74,
              "Pediatric Pulmonology": 70,
              "Addiction Medicine": 68,
              "Developmental-Behavioral Pediatrics": 66,
              "Surgery of the Hand": 64,
              "Child and Adolescent Psychiatry": 58,
              "Occupational and Environmental Medicine": 58,
              "Colon and Rectal Surgery": 54,
              "Advanced Heart Failure and Transplant Cardiology": 54,
              "Pathology - Medical Microbiology": 52,
              "Thoracic and Cardiac Surgery": 51,
              "Sports Medicine": 51,
              "Vascular Neurology": 50,
              "Neurotology": 50,
              "Complex Pediatric Otolaryngology": 45,
              "History": 45,
              "Orthopaedic Sports Medicine": 42,
              "Diagnostic Radiology": 42,
              "Physical Medicine and Rehabilitation": 41,
              "Complex General Surgical Oncology": 39,
              "Sleep Medicine": 39,
              "Pediatric Emergency Medicine": 38,
              "Pediatric Urology": 36,
              "Surgical Critical Care": 33,
              "Neurocritical Care": 33,
              "Astronomy": 33,
              "Geriatric Medicine": 32,
              "Pediatric Critical Care Medicine": 32,
              "Nuclear Medicine": 31,
              "Urogynecology and Reconstructive Pelvic Surgery": 30,
              "Neuropathology": 29,
              "Transplant Hepatology": 27,
              "Aerospace Medicine": 27,
              "Plastic Surgery": 25,
              "Pediatric Anesthesiology": 25,
              "Politics": 24,
              "Pathology - Anatomic/Pathology - Clinical": 24,
              "Pain Medicine": 23,
              "Undersea and Hyperbaric Medicine": 22,
              "Adolescent Medicine": 22,
              "Pediatric Rheumatology": 21,
              "Hospice and Palliative Medicine": 21,
              "Molecular Genetic Pathology": 20,
              "Medical Biochemical Genetics": 20,
              "Radiation Oncology": 18,
              "Interventional Cardiology": 18,
              "Hematopathology": 17,
              "Dermatopathology": 17,
              "Pathology - Pediatric": 14,
              "Complex Family Planning": 14,
              "Adult Cardiac Anesthesiology": 14,
              "Laboratory Genetics and Genomics": 14,
              "Neuroradiology": 12,
              "Adult Congenital Heart Disease": 12,
              "Blood Banking/Transfusion Medicine": 12,
              "Pathology - Clinical": 10,
              "Podiatry": 10,
              "Child Abuse Pediatrics": 9,
              "Plastic Surgery Within the Head and Neck*": 6,
              "Cytopathology": 6,
              "Spinal Cord Injury Medicine": 6,
              "Neuromuscular Medicine": 5,
              "Internal Medicine-Critical Care Medicine": 5,
              "Geography": 5,
              "Religion": 4,
              "Pediatric Radiology": 4,
              "Interventional Radiology and Diagnostic Radiology": 4,
              "Clinical Neurophysiology": 4,
              "Geriatric Psychiatry": 4,
              "Addiction Psychiatry": 3,
              "Emergency Medical Services": 3,
              "Consultation-Liaison Psychiatry": 3,
              "Optometry": 3,
              "Epilepsy": 2,
              "Pediatric Rehabilitation Medicine": 2,
              "Pathology - Molecular Genetic": 2,
              "Neurodevelopmental Disabilities": 2,
              "Nuclear Radiology": 1,
              "Biology": 1,
              "Congenital Cardiac Surgery": 1
            }
          }
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/med-s1-25k",
          "timestamp": "2025-02-28T00:02:23Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-25k/med-s1-25keval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-25k/med-s1-25keval_data_metrics.json",
          "timestamp": "2025-03-01T06:40:17.940429",
          "summary": {
            "MedMCQA_validation": {
              "accuracy": 0.5888118575185274,
              "accuracy_ci": [
                0.5715993306239541,
                0.6007650011953144
              ],
              "num_correct": 2455,
              "total_examples": 4183,
              "num_answered": 2463
            },
            "MedQA_USLME_test": {
              "accuracy": 0.7014925373134329,
              "accuracy_ci": [
                0.6763550667714061,
                0.7266496465043204
              ],
              "num_correct": 893,
              "total_examples": 1273,
              "num_answered": 882
            },
            "PubMedQA_test": {
              "accuracy": 0.777,
              "accuracy_ci": [
                0.7479750000000001,
                0.799
              ],
              "num_correct": 775,
              "total_examples": 1000,
              "num_answered": 777
            },
            "MMLU-Pro_Medical_test": {
              "accuracy": 0.6058631921824105,
              "accuracy_ci": [
                0.5824104234527687,
                0.628029315960912
              ],
              "num_correct": 930,
              "total_examples": 1535,
              "num_answered": 930
            },
            "GPQA_Medical_test": {
              "accuracy": 0.47435897435897434,
              "accuracy_ci": [
                0.4076923076923077,
                0.5051282051282051
              ],
              "num_correct": 179,
              "total_examples": 390,
              "num_answered": 185
            }
          }
        }
      }
    },
    "random-1k": {
      "description": "Random sampling with 1k samples",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "n_samples": 1000
        },
        "training_params": {
          "learning_rate": 2e-06,
          "batch_size": 16,
          "num_epochs": 5,
          "weight_decay": 0.001
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/random-1k_20250227_045807/med_s1k_formatted",
          "timestamp": "2025-02-27T04:58:09.174433",
          "stats": {
            "total_examples": 25371,
            "selected_examples": 1000,
            "filtered_examples": 25329,
            "filter_reasons": {
              "not_selected_in_sampling": 25049,
              "base_model_correct": 280
            },
            "specialty_distribution": {
              "Psychiatry": 31,
              "Neurology": 28,
              "Dentistry": 28,
              "Physics": 27,
              "Infectious Disease": 26,
              "Obstetrics and Gynecology": 26,
              "General Knowledge": 25,
              "Math": 24,
              "Pulmonary Disease": 23,
              "Gastroenterology": 23,
              "Endocrinology, Diabetes and Metabolism": 19,
              "Public Health and General Preventive Medicine": 14,
              "Otolaryngology - Head and Neck Surgery": 14,
              "Hematology": 14,
              "Clinical Genetics and Genomics (MD)": 13,
              "Pediatric Infectious Diseases": 12,
              "Nephrology": 12,
              "Cardiovascular Disease": 12,
              "Health Care Administration, Leadership, and Management": 11,
              "Ophthalmology": 11,
              "Computer Science": 10,
              "Maternal-Fetal Medicine": 10,
              "Rheumatology": 10,
              "Dermatology": 9,
              "Medical Toxicology": 9,
              "Law": 9,
              "General Surgery": 9,
              "Gynecologic Oncology": 9,
              "Neonatal-Perinatal Medicine": 9,
              "Anesthesiology": 9,
              "Economics": 8,
              "Orthopaedic Surgery": 8,
              "Clinical Biochemical Genetics": 7,
              "Clinical Cardiac Electrophysiology": 7,
              "Allergy and Immunology": 7,
              "Urology": 7,
              "Internal Medicine": 7,
              "Emergency Medicine": 7,
              "Reproductive Endocrinology and Infertility": 6,
              "Vascular Surgery": 6,
              "Chemistry": 6,
              "Pathology - Anatomic": 6,
              "Pediatric Nephrology": 6,
              "Pediatrics": 6,
              "Pediatric Surgery": 6,
              "Pathology - Forensic": 5,
              "Pediatric Urology": 5,
              "Clinical Informatics": 5,
              "Pediatric Gastroenterology": 5,
              "Pediatric Endocrinology": 5,
              "Critical Care Medicine": 5,
              "Medical Physics (Diagnostic, Nuclear, Therapeutic)": 5,
              "Pediatric Hematology-Oncology": 5,
              "Pathology - Chemical": 4,
              "Addiction Medicine": 4,
              "Pediatric Cardiology": 3,
              "Astronomy": 3,
              "Complex Pediatric Otolaryngology": 3,
              "Developmental-Behavioral Pediatrics": 3,
              "Family Medicine": 3,
              "Complex General Surgical Oncology": 3,
              "Medical Oncology": 3,
              "Education": 3,
              "Colon and Rectal Surgery": 3,
              "Transplant Hepatology": 3,
              "Pediatric Emergency Medicine": 3,
              "Adult Congenital Heart Disease": 2,
              "Advanced Heart Failure and Transplant Cardiology": 2,
              "Pathology - Medical Microbiology": 2,
              "Sports Medicine": 2,
              "Forensic Psychiatry": 2,
              "Pathology - Clinical": 2,
              "Vascular Neurology": 2,
              "Neurotology": 2,
              "Urogynecology and Reconstructive Pelvic Surgery": 2,
              "Neurological Surgery": 2,
              "Undersea and Hyperbaric Medicine": 2,
              "Diagnostic Radiology": 2,
              "Pediatric Dermatology": 2,
              "Physical Medicine and Rehabilitation": 2,
              "Podiatry": 1,
              "Plastic Surgery": 1,
              "Pathology - Anatomic/Pathology - Clinical": 1,
              "Pediatric Critical Care Medicine": 1,
              "Child and Adolescent Psychiatry": 1,
              "Occupational and Environmental Medicine": 1,
              "Medical Biochemical Genetics": 1,
              "Geriatric Medicine": 1,
              "History": 1,
              "Statistics": 1,
              "Pain Medicine": 1,
              "Laboratory Genetics and Genomics": 1,
              "Pathology - Pediatric": 1,
              "Aerospace Medicine": 1,
              "Neuropathology": 1,
              "Hematopathology": 1,
              "Religion": 1,
              "Orthopaedic Sports Medicine": 1,
              "Neurocritical Care": 1,
              "Radiation Oncology": 1,
              "Politics": 1,
              "Surgical Critical Care": 1,
              "Neuromuscular Medicine": 1,
              "Surgery of the Hand": 1,
              "Consultation-Liaison Psychiatry": 1
            }
          }
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/random-1k",
          "timestamp": "2025-02-27T18:42:32Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/random-1k/random-1keval_data_strict-prompt.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/random-1k/random-1keval_data_strict-prompt_metrics.json",
          "timestamp": "2025-02-27T20:34:37.710394",
          "summary": {
            "MedMCQA_validation": {
              "accuracy": 0.5761415252211332,
              "accuracy_ci": [
                0.5606024384413101,
                0.5902462347597418
              ],
              "num_correct": 2409,
              "total_examples": 4183,
              "num_answered": 2410
            },
            "MedQA_USLME_test": {
              "accuracy": 0.6669285153181461,
              "accuracy_ci": [
                0.6417910447761194,
                0.6936370777690495
              ],
              "num_correct": 849,
              "total_examples": 1273,
              "num_answered": 843
            },
            "PubMedQA_test": {
              "accuracy": 0.773,
              "accuracy_ci": [
                0.7469750000000001,
                0.799025
              ],
              "num_correct": 773,
              "total_examples": 1000,
              "num_answered": 772
            },
            "MMLU-Pro_Medical_test": {
              "accuracy": 0.5700325732899023,
              "accuracy_ci": [
                0.545928338762215,
                0.5948045602605863
              ],
              "num_correct": 875,
              "total_examples": 1535,
              "num_answered": 871
            },
            "GPQA_Medical_test": {
              "accuracy": 0.48205128205128206,
              "accuracy_ci": [
                0.43333333333333335,
                0.5333333333333333
              ],
              "num_correct": 188,
              "total_examples": 390,
              "num_answered": 186
            }
          }
        }
      }
    },
    "random-5k": {
      "description": "Random sampling with 5k samples",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": {
          "method": "random",
          "n_samples": 5000
        },
        "training_params": {
          "learning_rate": 5e-06,
          "batch_size": 32,
          "num_epochs": 3,
          "weight_decay": 0.001
        }
      },
      "results": {
        "curation": {
          "dataset_path": "/share/pi/nigam/users/calebwin/hf_cache/med-s1k/random-5k_20250227_134720/med_s1k_formatted",
          "timestamp": "2025-02-27T13:47:22.752129",
          "stats": {
            "total_examples": 25371,
            "selected_examples": 5000,
            "filtered_examples": 25170,
            "filter_reasons": {
              "not_selected_in_sampling": 23768,
              "base_model_correct": 1402
            },
            "specialty_distribution": {
              "Infectious Disease": 154,
              "General Knowledge": 148,
              "Gastroenterology": 145,
              "Neurology": 145,
              "Math": 112,
              "Psychiatry": 110,
              "Dentistry": 104,
              "Obstetrics and Gynecology": 103,
              "Endocrinology, Diabetes and Metabolism": 98,
              "Physics": 97,
              "Pulmonary Disease": 86,
              "Ophthalmology": 81,
              "Public Health and General Preventive Medicine": 76,
              "Cardiovascular Disease": 76,
              "Otolaryngology - Head and Neck Surgery": 75,
              "Nephrology": 75,
              "Health Care Administration, Leadership, and Management": 68,
              "Maternal-Fetal Medicine": 65,
              "Hematology": 61,
              "Pediatrics": 57,
              "Dermatology": 53,
              "General Surgery": 51,
              "Economics": 51,
              "Rheumatology": 48,
              "Internal Medicine": 47,
              "Clinical Genetics and Genomics (MD)": 46,
              "Pediatric Infectious Diseases": 45,
              "Orthopaedic Surgery": 45,
              "Anesthesiology": 44,
              "Medical Toxicology": 43,
              "Law": 42,
              "Urology": 39,
              "Computer Science": 36,
              "Allergy and Immunology": 35,
              "Medical Physics (Diagnostic, Nuclear, Therapeutic)": 34,
              "Neonatal-Perinatal Medicine": 34,
              "Medical Oncology": 32,
              "Emergency Medicine": 32,
              "Clinical Informatics": 31,
              "Gynecologic Oncology": 31,
              "Pediatric Hematology-Oncology": 28,
              "Clinical Biochemical Genetics": 28,
              "Vascular Surgery": 28,
              "Pediatric Endocrinology": 27,
              "Pathology - Forensic": 27,
              "Chemistry": 26,
              "Reproductive Endocrinology and Infertility": 25,
              "Pathology - Chemical": 24,
              "Critical Care Medicine": 24,
              "Pathology - Anatomic": 23,
              "Pediatric Nephrology": 23,
              "Pediatric Gastroenterology": 22,
              "Neurology with Special Qualification in Child Neurology": 22,
              "Education": 22,
              "Statistics": 19,
              "Forensic Psychiatry": 19,
              "Clinical Cardiac Electrophysiology": 19,
              "Neurological Surgery": 18,
              "Occupational and Environmental Medicine": 16,
              "Pediatric Cardiology": 16,
              "Pediatric Surgery": 15,
              "Pediatric Dermatology": 15,
              "Addiction Medicine": 15,
              "Pediatric Pulmonology": 14,
              "Neurotology": 14,
              "Vascular Neurology": 13,
              "Family Medicine": 13,
              "Advanced Heart Failure and Transplant Cardiology": 12,
              "Surgery of the Hand": 12,
              "Orthopaedic Sports Medicine": 11,
              "Colon and Rectal Surgery": 11,
              "Pediatric Emergency Medicine": 10,
              "Pathology - Medical Microbiology": 10,
              "Sports Medicine": 9,
              "Neuropathology": 9,
              "Sleep Medicine": 9,
              "Astronomy": 9,
              "Geriatric Medicine": 9,
              "Complex Pediatric Otolaryngology": 8,
              "Urogynecology and Reconstructive Pelvic Surgery": 8,
              "Thoracic and Cardiac Surgery": 8,
              "Politics": 7,
              "Physical Medicine and Rehabilitation": 7,
              "Child and Adolescent Psychiatry": 7,
              "Pathology - Anatomic/Pathology - Clinical": 7,
              "Complex General Surgical Oncology": 7,
              "Pediatric Urology": 6,
              "Undersea and Hyperbaric Medicine": 6,
              "Surgical Critical Care": 6,
              "Radiation Oncology": 6,
              "History": 6,
              "Adolescent Medicine": 5,
              "Pediatric Critical Care Medicine": 5,
              "Diagnostic Radiology": 5,
              "Developmental-Behavioral Pediatrics": 5,
              "Plastic Surgery": 4,
              "Transplant Hepatology": 4,
              "Adult Congenital Heart Disease": 4,
              "Pediatric Anesthesiology": 4,
              "Laboratory Genetics and Genomics": 4,
              "Pathology - Clinical": 3,
              "Hospice and Palliative Medicine": 3,
              "Aerospace Medicine": 3,
              "Pediatric Rheumatology": 3,
              "Neuroradiology": 3,
              "Hematopathology": 3,
              "Adult Cardiac Anesthesiology": 3,
              "Medical Biochemical Genetics": 2,
              "Nuclear Medicine": 2,
              "Spinal Cord Injury Medicine": 2,
              "Child Abuse Pediatrics": 2,
              "Complex Family Planning": 2,
              "Podiatry": 2,
              "Pathology - Pediatric": 2,
              "Pain Medicine": 2,
              "Neurodevelopmental Disabilities": 1,
              "Interventional Radiology and Diagnostic Radiology": 1,
              "Dermatopathology": 1,
              "Neurocritical Care": 1,
              "Religion": 1,
              "Pathology - Molecular Genetic": 1,
              "Blood Banking/Transfusion Medicine": 1,
              "Pediatric Radiology": 1,
              "Molecular Genetic Pathology": 1,
              "Neuromuscular Medicine": 1,
              "Epilepsy": 1,
              "Consultation-Liaison Psychiatry": 1
            }
          }
        },
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/random-5k",
          "timestamp": "2025-02-27T20:18:37Z",
          "metrics": null
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/random-5k/random-5keval_data_strict-prompt.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/random-5k/random-5keval_data_strict-prompt_metrics.json",
          "timestamp": "2025-02-27T21:01:33.077143",
          "summary": {
            "MedMCQA_validation": {
              "accuracy": 0.5804446569447764,
              "accuracy_ci": [
                0.5648995935931149,
                0.5947884293569209
              ],
              "num_correct": 2428,
              "total_examples": 4183,
              "num_answered": 2428
            },
            "MedQA_USLME_test": {
              "accuracy": 0.6645718774548312,
              "accuracy_ci": [
                0.6394344069128044,
                0.6912804399057345
              ],
              "num_correct": 846,
              "total_examples": 1273,
              "num_answered": 846
            },
            "PubMedQA_test": {
              "accuracy": 0.768,
              "accuracy_ci": [
                0.741,
                0.795
              ],
              "num_correct": 768,
              "total_examples": 1000,
              "num_answered": 768
            },
            "MMLU-Pro_Medical_test": {
              "accuracy": 0.5830618892508144,
              "accuracy_ci": [
                0.5595928338762215,
                0.6091205211726385
              ],
              "num_correct": 895,
              "total_examples": 1535,
              "num_answered": 894
            },
            "GPQA_Medical_test": {
              "accuracy": 0.48717948717948717,
              "accuracy_ci": [
                0.43846153846153846,
                0.5384615384615384
              ],
              "num_correct": 190,
              "total_examples": 390,
              "num_answered": 188
            }
          }
        }
      }
    },
    "base": {
      "description": "Base LLaMA model without fine-tuning",
      "config": {
        "model_key": "llama3.1:8b",
        "curation": null,
        "training_params": null
      },
      "results": {
        "curation": null,
        "training": null,
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base/Llama-3.1-8B-Instructeval_data_strict-prompt.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base/Llama-3.1-8B-Instructeval_data_strict-prompt_metrics.json",
          "timestamp": "2025-02-26T18:18:27.504904",
          "summary": {
            "MedMCQA_validation": {
              "accuracy": 0.5541477408558451,
              "accuracy_ci": [
                0.5378854888835763,
                0.568497489839828
              ],
              "num_correct": 2318,
              "total_examples": 4183,
              "num_answered": 2318
            },
            "MedQA_USLME_test": {
              "accuracy": 0.6284367635506677,
              "accuracy_ci": [
                0.6017085624509034,
                0.6520227808326786
              ],
              "num_correct": 799,
              "total_examples": 1273,
              "num_answered": 800
            },
            "PubMedQA_test": {
              "accuracy": 0.753,
              "accuracy_ci": [
                0.727,
                0.781
              ],
              "num_correct": 753,
              "total_examples": 1000,
              "num_answered": 753
            },
            "MMLU-Pro_Medical_test": {
              "accuracy": 0.5954397394136808,
              "accuracy_ci": [
                0.5706840390879478,
                0.6188925081433225
              ],
              "num_correct": 912,
              "total_examples": 1535,
              "num_answered": 914
            },
            "GPQA_Medical_test": {
              "accuracy": 0.42597402597402595,
              "accuracy_ci": [
                0.37662337662337664,
                0.4753246753246753
              ],
              "num_correct": 164,
              "total_examples": 385,
              "num_answered": 163
            }
          }
        }
      }
    },
    "huatuo": {
      "description": "Pre-trained HuatuoGPT medical model",
      "config": {
        "model_key": "huatuo:8b",
        "curation": null,
        "training_params": null
      },
      "results": {
        "curation": null,
        "training": null,
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo/HuatuoGPT-o1-8Beval_data_strict-prompt.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo/HuatuoGPT-o1-8Beval_data_strict-prompt_metrics.json",
          "timestamp": "2025-02-26T19:25:08.802980",
          "summary": {
            "MedMCQA_validation": {
              "accuracy": 0.6323213005020321,
              "accuracy_ci": [
                0.6177384652163519,
                0.6466650729141764
              ],
              "num_correct": 2645,
              "total_examples": 4183,
              "num_answered": 2645
            },
            "MedQA_USLME_test": {
              "accuracy": 0.7588373919874313,
              "accuracy_ci": [
                0.7329143754909663,
                0.7831893165750197
              ],
              "num_correct": 966,
              "total_examples": 1273,
              "num_answered": 966
            },
            "PubMedQA_test": {
              "accuracy": 0.802,
              "accuracy_ci": [
                0.777,
                0.826
              ],
              "num_correct": 802,
              "total_examples": 1000,
              "num_answered": 802
            },
            "MMLU-Pro_Medical_test": {
              "accuracy": 0.6488599348534202,
              "accuracy_ci": [
                0.6253908794788273,
                0.6729804560260586
              ],
              "num_correct": 996,
              "total_examples": 1535,
              "num_answered": 995
            },
            "GPQA_Medical_test": {
              "accuracy": 0.5506493506493506,
              "accuracy_ci": [
                0.5012987012987012,
                0.6025974025974026
              ],
              "num_correct": 212,
              "total_examples": 385,
              "num_answered": 212
            }
          }
        }
      }
    },
    "huatuo-eval-250": {
      "description": "Pre-trained HuatuoGPT medical model",
      "config": {
        "model_key": "huatuo:8b",
        "curation": null,
        "training_params": null,
        "eval_sample": 250
      },
      "results": {
        "curation": null,
        "training": null,
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo-eval-250/HuatuoGPT-o1-8Beval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo-eval-250/HuatuoGPT-o1-8Beval_data_metrics.json",
          "timestamp": "2025-03-01T01:57:35.670350",
          "summary": {
            "MedMCQA_validation": {
              "accuracy": 0.6421228783169973,
              "accuracy_ci": [
                0.627779105904853,
                0.6569447764762133
              ],
              "num_correct": 2686,
              "total_examples": 4183,
              "num_answered": 2685
            },
            "MedQA_USLME_test": {
              "accuracy": 0.7690494893951296,
              "accuracy_ci": [
                0.7470542026708562,
                0.7918303220738413
              ],
              "num_correct": 979,
              "total_examples": 1273,
              "num_answered": 969
            },
            "PubMedQA_test": {
              "accuracy": 0.797,
              "accuracy_ci": [
                0.7729750000000001,
                0.823
              ],
              "num_correct": 797,
              "total_examples": 1000,
              "num_answered": 796
            },
            "MMLU-Pro_Medical_test": {
              "accuracy": 0.6599348534201954,
              "accuracy_ci": [
                0.6351791530944625,
                0.6814332247557003
              ],
              "num_correct": 1009,
              "total_examples": 1535,
              "num_answered": 1013
            },
            "GPQA_Medical_test": {
              "accuracy": 0.5666666666666667,
              "accuracy_ci": [
                0.5128205128205128,
                0.6102564102564103
              ],
              "num_correct": 220,
              "total_examples": 390,
              "num_answered": 221
            }
          }
        }
      }
    },
    "huatuo-eval-2000": {
      "description": "Pre-trained HuatuoGPT medical model",
      "config": {
        "model_key": "huatuo:8b",
        "curation": null,
        "training_params": null,
        "eval_sample": 2000
      },
      "results": {
        "curation": null,
        "training": null,
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo-eval-2000/HuatuoGPT-o1-8Beval_data.json",
          "metrics_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo-eval-2000/HuatuoGPT-o1-8Beval_data_metrics.json",
          "timestamp": "2025-03-01T01:59:25.359007",
          "summary": {
            "MedMCQA_validation": {
              "accuracy": 0.638775998087497,
              "accuracy_ci": [
                0.6241931628018169,
                0.6528807076261056
              ],
              "num_correct": 2672,
              "total_examples": 4183,
              "num_answered": 2652
            },
            "MedQA_USLME_test": {
              "accuracy": 0.762765121759623,
              "accuracy_ci": [
                0.7391987431264729,
                0.786331500392773
              ],
              "num_correct": 971,
              "total_examples": 1273,
              "num_answered": 964
            },
            "PubMedQA_test": {
              "accuracy": 0.799,
              "accuracy_ci": [
                0.7719750000000001,
                0.822
              ],
              "num_correct": 797,
              "total_examples": 1000,
              "num_answered": 799
            },
            "MMLU-Pro_Medical_test": {
              "accuracy": 0.6488599348534202,
              "accuracy_ci": [
                0.6260586319218241,
                0.6736156351791531
              ],
              "num_correct": 996,
              "total_examples": 1535,
              "num_answered": 990
            },
            "GPQA_Medical_test": {
              "accuracy": 0.5538461538461539,
              "accuracy_ci": [
                0.49224358974358984,
                0.5923076923076923
              ],
              "num_correct": 211,
              "total_examples": 390,
              "num_answered": 216
            }
          }
        }
      }
    },
    "med-s1-1k-tuned-tts": {
      "description": "Test time scaling evaluation of med-s1-1k-tuned",
      "config": {
        "test_time_scaling": true,
        "method": "med-s1-1k-tuned",
        "eval_sample": 2000
      },
      "results": {
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/med-s1-1k-tuned"
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-1k-tuned-tts/med-s1-1k-tunedeval_data.json",
          "timestamp": "2025-03-01T04:30:14.200418",
          "test_time_scaling": true,
          "summary": {
            "immediate": {
              "MedMCQA_validation": {
                "accuracy": 0.5704365079365079,
                "accuracy_ci": [
                  0.5317460317460317,
                  0.5922867063492063
                ],
                "num_correct": 566,
                "total_examples": 1008,
                "num_answered": 575
              },
              "MedQA_USLME_test": {
                "accuracy": 0.625,
                "accuracy_ci": [
                  0.5723684210526315,
                  0.6776315789473685
                ],
                "num_correct": 190,
                "total_examples": 304,
                "num_answered": 190
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5854341736694678,
                "accuracy_ci": [
                  0.5350140056022409,
                  0.6414565826330533
                ],
                "num_correct": 209,
                "total_examples": 357,
                "num_answered": 208
              },
              "GPQA_Medical_test": {
                "accuracy": 0.375,
                "accuracy_ci": [
                  0.2375,
                  0.4375
                ],
                "num_correct": 27,
                "total_examples": 80,
                "num_answered": 30
              },
              "PubMedQA_test": {
                "accuracy": 0.7928286852589641,
                "accuracy_ci": [
                  0.7410358565737052,
                  0.8406374501992032
                ],
                "num_correct": 199,
                "total_examples": 251,
                "num_answered": 198
              }
            },
            "reasoning": {
              "MedMCQA_validation": {
                "accuracy": 0.5694444444444444,
                "accuracy_ci": [
                  0.4365079365079365,
                  0.49503968253968256
                ],
                "num_correct": 470,
                "total_examples": 1008,
                "num_answered": 574
              },
              "MedQA_USLME_test": {
                "accuracy": 0.680921052631579,
                "accuracy_ci": [
                  0.5230263157894737,
                  0.6348684210526315
                ],
                "num_correct": 176,
                "total_examples": 304,
                "num_answered": 207
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6190476190476191,
                "accuracy_ci": [
                  0.4481792717086835,
                  0.5518207282913166
                ],
                "num_correct": 178,
                "total_examples": 357,
                "num_answered": 221
              },
              "GPQA_Medical_test": {
                "accuracy": 0.4375,
                "accuracy_ci": [
                  0.3,
                  0.5125
                ],
                "num_correct": 32,
                "total_examples": 80,
                "num_answered": 35
              },
              "PubMedQA_test": {
                "accuracy": 0.7689243027888446,
                "accuracy_ci": [
                  0.5139442231075697,
                  0.6374501992031872
                ],
                "num_correct": 146,
                "total_examples": 251,
                "num_answered": 193
              }
            },
            "reasoning_2x": {
              "MedMCQA_validation": {
                "accuracy": 0.564484126984127,
                "accuracy_ci": [
                  0.5337301587301587,
                  0.5922619047619048
                ],
                "num_correct": 566,
                "total_examples": 1008,
                "num_answered": 569
              },
              "MedQA_USLME_test": {
                "accuracy": 0.7006578947368421,
                "accuracy_ci": [
                  0.6480263157894737,
                  0.7467105263157895
                ],
                "num_correct": 213,
                "total_examples": 304,
                "num_answered": 213
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5938375350140056,
                "accuracy_ci": [
                  0.5434173669467787,
                  0.6470588235294118
                ],
                "num_correct": 212,
                "total_examples": 357,
                "num_answered": 211
              },
              "GPQA_Medical_test": {
                "accuracy": 0.4625,
                "accuracy_ci": [
                  0.35,
                  0.575
                ],
                "num_correct": 37,
                "total_examples": 80,
                "num_answered": 36
              },
              "PubMedQA_test": {
                "accuracy": 0.701195219123506,
                "accuracy_ci": [
                  0.6414342629482072,
                  0.7569721115537849
                ],
                "num_correct": 176,
                "total_examples": 251,
                "num_answered": 175
              }
            },
            "reasoning_4x": {
              "MedMCQA_validation": {
                "accuracy": 0.5674603174603174,
                "accuracy_ci": [
                  0.5327380952380952,
                  0.5952380952380952
                ],
                "num_correct": 568,
                "total_examples": 1008,
                "num_answered": 572
              },
              "MedQA_USLME_test": {
                "accuracy": 0.680921052631579,
                "accuracy_ci": [
                  0.6282894736842105,
                  0.7335526315789473
                ],
                "num_correct": 207,
                "total_examples": 304,
                "num_answered": 204
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6078431372549019,
                "accuracy_ci": [
                  0.5546218487394958,
                  0.6554621848739496
                ],
                "num_correct": 216,
                "total_examples": 357,
                "num_answered": 217
              },
              "GPQA_Medical_test": {
                "accuracy": 0.4875,
                "accuracy_ci": [
                  0.3625,
                  0.5875
                ],
                "num_correct": 38,
                "total_examples": 80,
                "num_answered": 39
              },
              "PubMedQA_test": {
                "accuracy": 0.7609561752988048,
                "accuracy_ci": [
                  0.7130478087649403,
                  0.8127490039840638
                ],
                "num_correct": 191,
                "total_examples": 251,
                "num_answered": 191
              }
            }
          },
          "reasoning_tokens": {
            "immediate": 0,
            "reasoning": 399.4185,
            "reasoning_2x": 905.841,
            "reasoning_4x": 1405.2355
          },
          "test_time_scaling_plot": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-1k-tuned-tts/med-s1-1k-tunedeval_data_plot.png"
        }
      }
    },
    "random-1k-tts": {
      "description": "Test time scaling evaluation of random-1k",
      "config": {
        "test_time_scaling": true,
        "method": "random-1k",
        "eval_sample": 2000
      },
      "results": {
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/random-1k"
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/random-1k-tts/random-1keval_data.json",
          "timestamp": "2025-03-01T02:04:35.512854",
          "test_time_scaling": true,
          "summary": {
            "immediate": {
              "MedMCQA_validation": {
                "accuracy": 0.5843253968253969,
                "accuracy_ci": [
                  0.5456349206349206,
                  0.6051587301587301
                ],
                "num_correct": 579,
                "total_examples": 1008,
                "num_answered": 589
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6414473684210527,
                "accuracy_ci": [
                  0.5427631578947368,
                  0.6513157894736842
                ],
                "num_correct": 182,
                "total_examples": 304,
                "num_answered": 195
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5826330532212886,
                "accuracy_ci": [
                  0.5070028011204482,
                  0.6106442577030813
                ],
                "num_correct": 199,
                "total_examples": 357,
                "num_answered": 208
              },
              "GPQA_Medical_test": {
                "accuracy": 0.375,
                "accuracy_ci": [
                  0.225,
                  0.425
                ],
                "num_correct": 26,
                "total_examples": 80,
                "num_answered": 30
              },
              "PubMedQA_test": {
                "accuracy": 0.7569721115537849,
                "accuracy_ci": [
                  0.6972111553784861,
                  0.8048804780876493
                ],
                "num_correct": 189,
                "total_examples": 251,
                "num_answered": 190
              }
            },
            "reasoning": {
              "MedMCQA_validation": {
                "accuracy": 0.5734126984126984,
                "accuracy_ci": [
                  0.4494047619047619,
                  0.5138888888888888
                ],
                "num_correct": 486,
                "total_examples": 1008,
                "num_answered": 578
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6611842105263158,
                "accuracy_ci": [
                  0.5164473684210527,
                  0.6282894736842105
                ],
                "num_correct": 174,
                "total_examples": 304,
                "num_answered": 201
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6022408963585434,
                "accuracy_ci": [
                  0.37808123249299724,
                  0.4789915966386555
                ],
                "num_correct": 153,
                "total_examples": 357,
                "num_answered": 215
              },
              "GPQA_Medical_test": {
                "accuracy": 0.45,
                "accuracy_ci": [
                  0.325,
                  0.5375
                ],
                "num_correct": 35,
                "total_examples": 80,
                "num_answered": 36
              },
              "PubMedQA_test": {
                "accuracy": 0.7370517928286853,
                "accuracy_ci": [
                  0.5179282868525896,
                  0.6334661354581673
                ],
                "num_correct": 145,
                "total_examples": 251,
                "num_answered": 185
              }
            },
            "reasoning_2x": {
              "MedMCQA_validation": {
                "accuracy": 0.5853174603174603,
                "accuracy_ci": [
                  0.5545634920634921,
                  0.6170634920634921
                ],
                "num_correct": 590,
                "total_examples": 1008,
                "num_answered": 589
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6348684210526315,
                "accuracy_ci": [
                  0.5789473684210527,
                  0.6875
                ],
                "num_correct": 193,
                "total_examples": 304,
                "num_answered": 192
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6050420168067226,
                "accuracy_ci": [
                  0.5574229691876751,
                  0.6526610644257703
                ],
                "num_correct": 216,
                "total_examples": 357,
                "num_answered": 214
              },
              "GPQA_Medical_test": {
                "accuracy": 0.5125,
                "accuracy_ci": [
                  0.4,
                  0.625
                ],
                "num_correct": 41,
                "total_examples": 80,
                "num_answered": 41
              },
              "PubMedQA_test": {
                "accuracy": 0.749003984063745,
                "accuracy_ci": [
                  0.6892430278884463,
                  0.796812749003984
                ],
                "num_correct": 187,
                "total_examples": 251,
                "num_answered": 188
              }
            },
            "reasoning_4x": {
              "MedMCQA_validation": {
                "accuracy": 0.5942460317460317,
                "accuracy_ci": [
                  0.5634920634920635,
                  0.6240079365079365
                ],
                "num_correct": 598,
                "total_examples": 1008,
                "num_answered": 599
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6710526315789473,
                "accuracy_ci": [
                  0.6151315789473685,
                  0.7203947368421053
                ],
                "num_correct": 204,
                "total_examples": 304,
                "num_answered": 202
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6106442577030813,
                "accuracy_ci": [
                  0.5546218487394958,
                  0.6499299719887954
                ],
                "num_correct": 216,
                "total_examples": 357,
                "num_answered": 218
              },
              "GPQA_Medical_test": {
                "accuracy": 0.4,
                "accuracy_ci": [
                  0.2875,
                  0.5
                ],
                "num_correct": 31,
                "total_examples": 80,
                "num_answered": 32
              },
              "PubMedQA_test": {
                "accuracy": 0.7888446215139442,
                "accuracy_ci": [
                  0.7410358565737052,
                  0.8366533864541833
                ],
                "num_correct": 198,
                "total_examples": 251,
                "num_answered": 198
              }
            }
          },
          "reasoning_tokens": {
            "immediate": 0,
            "reasoning": 395.8265,
            "reasoning_2x": 772.9545,
            "reasoning_4x": 1168.3695
          },
          "test_time_scaling_plot": "/share/pi/nigam/users/calebwin/hf_cache/eval/random-1k-tts/random-1keval_data_plot.png"
        }
      }
    },
    "random-5k-tts": {
      "description": "Test time scaling evaluation of random-5k",
      "config": {
        "test_time_scaling": true,
        "method": "random-5k",
        "eval_sample": 2000
      },
      "results": {
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/random-5k"
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/random-5k-tts/random-5keval_data.json",
          "timestamp": "2025-03-01T19:20:58.026901",
          "test_time_scaling": true,
          "summary": {
            "immediate": {
              "MedMCQA_validation": {
                "accuracy": 0.5634920634920635,
                "accuracy_ci": [
                  0.5307539682539683,
                  0.5932787698412698
                ],
                "num_correct": 568,
                "total_examples": 1008,
                "num_answered": 561
              },
              "MedQA_USLME_test": {
                "accuracy": 0.5921052631578947,
                "accuracy_ci": [
                  0.5328947368421053,
                  0.6448190789473683
                ],
                "num_correct": 180,
                "total_examples": 304,
                "num_answered": 170
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5854341736694678,
                "accuracy_ci": [
                  0.5350140056022409,
                  0.6358543417366946
                ],
                "num_correct": 209,
                "total_examples": 357,
                "num_answered": 206
              },
              "GPQA_Medical_test": {
                "accuracy": 0.4625,
                "accuracy_ci": [
                  0.3625,
                  0.575
                ],
                "num_correct": 37,
                "total_examples": 80,
                "num_answered": 35
              },
              "PubMedQA_test": {
                "accuracy": 0.7768924302788844,
                "accuracy_ci": [
                  0.7250996015936255,
                  0.8286852589641435
                ],
                "num_correct": 195,
                "total_examples": 251,
                "num_answered": 195
              }
            },
            "reasoning": {
              "MedMCQA_validation": {
                "accuracy": 0.560515873015873,
                "accuracy_ci": [
                  0.5297619047619048,
                  0.5922619047619048
                ],
                "num_correct": 565,
                "total_examples": 1008,
                "num_answered": 565
              },
              "MedQA_USLME_test": {
                "accuracy": 0.555921052631579,
                "accuracy_ci": [
                  0.46710526315789475,
                  0.5888157894736842
                ],
                "num_correct": 160,
                "total_examples": 304,
                "num_answered": 169
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5826330532212886,
                "accuracy_ci": [
                  0.47619047619047616,
                  0.5827030812324929
                ],
                "num_correct": 189,
                "total_examples": 357,
                "num_answered": 208
              },
              "GPQA_Medical_test": {
                "accuracy": 0.45,
                "accuracy_ci": [
                  0.3125,
                  0.55
                ],
                "num_correct": 34,
                "total_examples": 80,
                "num_answered": 36
              },
              "PubMedQA_test": {
                "accuracy": 0.7569721115537849,
                "accuracy_ci": [
                  0.6972111553784861,
                  0.8007968127490039
                ],
                "num_correct": 188,
                "total_examples": 251,
                "num_answered": 190
              }
            },
            "reasoning_2x": {
              "MedMCQA_validation": {
                "accuracy": 0.5505952380952381,
                "accuracy_ci": [
                  0.5208333333333334,
                  0.581374007936508
                ],
                "num_correct": 555,
                "total_examples": 1008,
                "num_answered": 554
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6282894736842105,
                "accuracy_ci": [
                  0.569078947368421,
                  0.680921052631579
                ],
                "num_correct": 191,
                "total_examples": 304,
                "num_answered": 190
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5630252100840336,
                "accuracy_ci": [
                  0.5126050420168067,
                  0.6134453781512605
                ],
                "num_correct": 201,
                "total_examples": 357,
                "num_answered": 196
              },
              "GPQA_Medical_test": {
                "accuracy": 0.45,
                "accuracy_ci": [
                  0.2875,
                  0.5125
                ],
                "num_correct": 32,
                "total_examples": 80,
                "num_answered": 36
              },
              "PubMedQA_test": {
                "accuracy": 0.701195219123506,
                "accuracy_ci": [
                  0.6454183266932271,
                  0.7609561752988048
                ],
                "num_correct": 176,
                "total_examples": 251,
                "num_answered": 175
              }
            },
            "reasoning_4x": {
              "MedMCQA_validation": {
                "accuracy": 0.5446428571428571,
                "accuracy_ci": [
                  0.5099206349206349,
                  0.5724206349206349
                ],
                "num_correct": 546,
                "total_examples": 1008,
                "num_answered": 549
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6447368421052632,
                "accuracy_ci": [
                  0.5888157894736842,
                  0.7039473684210527
                ],
                "num_correct": 196,
                "total_examples": 304,
                "num_answered": 194
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5350140056022409,
                "accuracy_ci": [
                  0.48179271708683474,
                  0.5826330532212886
                ],
                "num_correct": 190,
                "total_examples": 357,
                "num_answered": 191
              },
              "GPQA_Medical_test": {
                "accuracy": 0.4125,
                "accuracy_ci": [
                  0.2625,
                  0.475
                ],
                "num_correct": 29,
                "total_examples": 80,
                "num_answered": 33
              },
              "PubMedQA_test": {
                "accuracy": 0.6892430278884463,
                "accuracy_ci": [
                  0.6334661354581673,
                  0.7451195219123505
                ],
                "num_correct": 173,
                "total_examples": 251,
                "num_answered": 173
              }
            }
          },
          "reasoning_tokens": {
            "immediate": 0,
            "reasoning": 439.802,
            "reasoning_2x": 1052.414,
            "reasoning_4x": 1727.092
          },
          "test_time_scaling_plot": "/share/pi/nigam/users/calebwin/hf_cache/eval/random-5k-tts/random-5keval_data_plot.png"
        }
      }
    },
    "med-s1-5k-tts": {
      "description": "Test time scaling evaluation of med-s1-5k",
      "config": {
        "test_time_scaling": true,
        "method": "med-s1-5k",
        "eval_sample": 2000
      },
      "results": {
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/med-s1-5k"
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-5k-tts/med-s1-5keval_data.json",
          "timestamp": "2025-03-01T09:09:07.831286",
          "test_time_scaling": true,
          "summary": {
            "immediate": {
              "MedMCQA_validation": {
                "accuracy": 0.5456349206349206,
                "accuracy_ci": [
                  0.5138888888888888,
                  0.5783978174603175
                ],
                "num_correct": 550,
                "total_examples": 1008,
                "num_answered": 548
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6085526315789473,
                "accuracy_ci": [
                  0.5592105263157895,
                  0.6644736842105263
                ],
                "num_correct": 185,
                "total_examples": 304,
                "num_answered": 174
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6022408963585434,
                "accuracy_ci": [
                  0.5490196078431373,
                  0.6498599439775911
                ],
                "num_correct": 215,
                "total_examples": 357,
                "num_answered": 204
              },
              "GPQA_Medical_test": {
                "accuracy": 0.425,
                "accuracy_ci": [
                  0.3125,
                  0.525
                ],
                "num_correct": 34,
                "total_examples": 80,
                "num_answered": 34
              },
              "PubMedQA_test": {
                "accuracy": 0.7529880478087649,
                "accuracy_ci": [
                  0.6972111553784861,
                  0.8007968127490039
                ],
                "num_correct": 188,
                "total_examples": 251,
                "num_answered": 189
              }
            },
            "reasoning": {
              "MedMCQA_validation": {
                "accuracy": 0.5069444444444444,
                "accuracy_ci": [
                  0.44742063492063494,
                  0.5089285714285714
                ],
                "num_correct": 482,
                "total_examples": 1008,
                "num_answered": 511
              },
              "MedQA_USLME_test": {
                "accuracy": 0.5526315789473685,
                "accuracy_ci": [
                  0.4276315789473684,
                  0.5460526315789473
                ],
                "num_correct": 148,
                "total_examples": 304,
                "num_answered": 168
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5210084033613446,
                "accuracy_ci": [
                  0.3473389355742297,
                  0.4481792717086835
                ],
                "num_correct": 142,
                "total_examples": 357,
                "num_answered": 186
              },
              "GPQA_Medical_test": {
                "accuracy": 0.375,
                "accuracy_ci": [
                  0.275,
                  0.4875
                ],
                "num_correct": 30,
                "total_examples": 80,
                "num_answered": 28
              },
              "PubMedQA_test": {
                "accuracy": 0.6573705179282868,
                "accuracy_ci": [
                  0.5577689243027888,
                  0.6734063745019919
                ],
                "num_correct": 155,
                "total_examples": 251,
                "num_answered": 165
              }
            },
            "reasoning_2x": {
              "MedMCQA_validation": {
                "accuracy": 0.5595238095238095,
                "accuracy_ci": [
                  0.5257936507936508,
                  0.5902777777777778
                ],
                "num_correct": 564,
                "total_examples": 1008,
                "num_answered": 560
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6381578947368421,
                "accuracy_ci": [
                  0.5822368421052632,
                  0.6907894736842105
                ],
                "num_correct": 194,
                "total_examples": 304,
                "num_answered": 191
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5630252100840336,
                "accuracy_ci": [
                  0.49859943977591037,
                  0.6022408963585434
                ],
                "num_correct": 197,
                "total_examples": 357,
                "num_answered": 201
              },
              "GPQA_Medical_test": {
                "accuracy": 0.5,
                "accuracy_ci": [
                  0.3625,
                  0.5875
                ],
                "num_correct": 38,
                "total_examples": 80,
                "num_answered": 40
              },
              "PubMedQA_test": {
                "accuracy": 0.7171314741035857,
                "accuracy_ci": [
                  0.6613545816733067,
                  0.7689243027888446
                ],
                "num_correct": 180,
                "total_examples": 251,
                "num_answered": 180
              }
            },
            "reasoning_4x": {
              "MedMCQA_validation": {
                "accuracy": 0.5367063492063492,
                "accuracy_ci": [
                  0.5069444444444444,
                  0.5664930555555555
                ],
                "num_correct": 541,
                "total_examples": 1008,
                "num_answered": 538
              },
              "MedQA_USLME_test": {
                "accuracy": 0.618421052631579,
                "accuracy_ci": [
                  0.5592105263157895,
                  0.6644736842105263
                ],
                "num_correct": 186,
                "total_examples": 304,
                "num_answered": 188
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5686274509803921,
                "accuracy_ci": [
                  0.5210084033613446,
                  0.6190476190476191
                ],
                "num_correct": 203,
                "total_examples": 357,
                "num_answered": 200
              },
              "GPQA_Medical_test": {
                "accuracy": 0.4,
                "accuracy_ci": [
                  0.3,
                  0.5125
                ],
                "num_correct": 32,
                "total_examples": 80,
                "num_answered": 31
              },
              "PubMedQA_test": {
                "accuracy": 0.7211155378486056,
                "accuracy_ci": [
                  0.6573705179282868,
                  0.77300796812749
                ],
                "num_correct": 181,
                "total_examples": 251,
                "num_answered": 181
              }
            }
          },
          "reasoning_tokens": {
            "immediate": 0,
            "reasoning": 481.839,
            "reasoning_2x": 1334.2095,
            "reasoning_4x": 2314.0855
          },
          "test_time_scaling_plot": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-5k-tts/med-s1-5keval_data_plot.png"
        }
      }
    },
    "med-s1-25k-tts": {
      "description": "Test time scaling evaluation of med-s1-25k",
      "config": {
        "test_time_scaling": true,
        "method": "med-s1-25k",
        "eval_sample": 2000
      },
      "results": {
        "training": {
          "model_path": "/share/pi/nigam/users/calebwin/hf_cache/ckpts/med-s1-25k"
        },
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-25k-tts/med-s1-25keval_data.json",
          "timestamp": "2025-03-01T08:13:14.517439",
          "test_time_scaling": true,
          "summary": {
            "immediate": {
              "MedMCQA_validation": {
                "accuracy": 0.5833333333333334,
                "accuracy_ci": [
                  0.5486111111111112,
                  0.6061755952380952
                ],
                "num_correct": 582,
                "total_examples": 1008,
                "num_answered": 588
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6546052631578947,
                "accuracy_ci": [
                  0.5953947368421053,
                  0.6973684210526315
                ],
                "num_correct": 196,
                "total_examples": 304,
                "num_answered": 199
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5798319327731093,
                "accuracy_ci": [
                  0.5294117647058824,
                  0.6302521008403361
                ],
                "num_correct": 207,
                "total_examples": 357,
                "num_answered": 207
              },
              "GPQA_Medical_test": {
                "accuracy": 0.45,
                "accuracy_ci": [
                  0.2875,
                  0.5125
                ],
                "num_correct": 32,
                "total_examples": 80,
                "num_answered": 36
              },
              "PubMedQA_test": {
                "accuracy": 0.7410358565737052,
                "accuracy_ci": [
                  0.6892430278884463,
                  0.796812749003984
                ],
                "num_correct": 186,
                "total_examples": 251,
                "num_answered": 186
              }
            },
            "reasoning": {
              "MedMCQA_validation": {
                "accuracy": 0.6001984126984127,
                "accuracy_ci": [
                  0.5704365079365079,
                  0.6279761904761905
                ],
                "num_correct": 604,
                "total_examples": 1008,
                "num_answered": 605
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6973684210526315,
                "accuracy_ci": [
                  0.6447368421052632,
                  0.75
                ],
                "num_correct": 212,
                "total_examples": 304,
                "num_answered": 210
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6050420168067226,
                "accuracy_ci": [
                  0.5573529411764707,
                  0.6554621848739496
                ],
                "num_correct": 216,
                "total_examples": 357,
                "num_answered": 212
              },
              "GPQA_Medical_test": {
                "accuracy": 0.4625,
                "accuracy_ci": [
                  0.3125,
                  0.525
                ],
                "num_correct": 34,
                "total_examples": 80,
                "num_answered": 37
              },
              "PubMedQA_test": {
                "accuracy": 0.7888446215139442,
                "accuracy_ci": [
                  0.7370517928286853,
                  0.8366533864541833
                ],
                "num_correct": 198,
                "total_examples": 251,
                "num_answered": 198
              }
            },
            "reasoning_2x": {
              "MedMCQA_validation": {
                "accuracy": 0.5515873015873016,
                "accuracy_ci": [
                  0.5208333333333334,
                  0.5833333333333334
                ],
                "num_correct": 556,
                "total_examples": 1008,
                "num_answered": 549
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6447368421052632,
                "accuracy_ci": [
                  0.5756578947368421,
                  0.6842105263157895
                ],
                "num_correct": 192,
                "total_examples": 304,
                "num_answered": 196
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5602240896358543,
                "accuracy_ci": [
                  0.5042016806722689,
                  0.6106442577030813
                ],
                "num_correct": 199,
                "total_examples": 357,
                "num_answered": 200
              },
              "GPQA_Medical_test": {
                "accuracy": 0.4375,
                "accuracy_ci": [
                  0.2875,
                  0.5
                ],
                "num_correct": 31,
                "total_examples": 80,
                "num_answered": 35
              },
              "PubMedQA_test": {
                "accuracy": 0.7091633466135459,
                "accuracy_ci": [
                  0.6573705179282868,
                  0.7649402390438247
                ],
                "num_correct": 178,
                "total_examples": 251,
                "num_answered": 178
              }
            },
            "reasoning_4x": {
              "MedMCQA_validation": {
                "accuracy": 0.5684523809523809,
                "accuracy_ci": [
                  0.5406498015873016,
                  0.5972222222222222
                ],
                "num_correct": 572,
                "total_examples": 1008,
                "num_answered": 573
              },
              "MedQA_USLME_test": {
                "accuracy": 0.631578947368421,
                "accuracy_ci": [
                  0.5625,
                  0.6776315789473685
                ],
                "num_correct": 189,
                "total_examples": 304,
                "num_answered": 192
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5742296918767507,
                "accuracy_ci": [
                  0.5182072829131653,
                  0.6275210084033612
                ],
                "num_correct": 205,
                "total_examples": 357,
                "num_answered": 205
              },
              "GPQA_Medical_test": {
                "accuracy": 0.375,
                "accuracy_ci": [
                  0.2375,
                  0.4625
                ],
                "num_correct": 28,
                "total_examples": 80,
                "num_answered": 30
              },
              "PubMedQA_test": {
                "accuracy": 0.7410358565737052,
                "accuracy_ci": [
                  0.6852589641434262,
                  0.796812749003984
                ],
                "num_correct": 186,
                "total_examples": 251,
                "num_answered": 186
              }
            }
          },
          "reasoning_tokens": {
            "immediate": 0,
            "reasoning": 455.572,
            "reasoning_2x": 1100.346,
            "reasoning_4x": 1923.741
          },
          "test_time_scaling_plot": "/share/pi/nigam/users/calebwin/hf_cache/eval/med-s1-25k-tts/med-s1-25keval_data_plot.png"
        }
      }
    },
    "base-tts": {
      "description": "Test time scaling evaluation of base model",
      "config": {
        "test_time_scaling": true,
        "method": "base",
        "eval_sample": 2000,
        "model_key": "llama3.1:8b",
        "reasoning_approaches": [
          "reasoning",
          "reasoning_2x_random",
          "reasoning_4x_random"
        ]
      },
      "results": {
        "training": {},
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base-tts/Llama-3.1-8B-Instructeval_data.json",
          "timestamp": "2025-03-04T01:36:16.836569",
          "test_time_scaling": true,
          "summary": {
            "reasoning": {
              "MedMCQA_validation": {
                "accuracy": 0.5634920634920635,
                "accuracy_ci": [
                  0.5287698412698413,
                  0.5892857142857143
                ],
                "num_correct": 563,
                "total_examples": 1008,
                "num_answered": 568
              },
              "MedQA_USLME_test": {
                "accuracy": 0.4967105263157895,
                "accuracy_ci": [
                  0.3651315789473684,
                  0.4769736842105263
                ],
                "num_correct": 128,
                "total_examples": 304,
                "num_answered": 151
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5854341736694678,
                "accuracy_ci": [
                  0.4789915966386555,
                  0.5798319327731093
                ],
                "num_correct": 189,
                "total_examples": 357,
                "num_answered": 209
              },
              "GPQA_Medical_test": {
                "accuracy": 0.375,
                "accuracy_ci": [
                  0.2625,
                  0.475
                ],
                "num_correct": 30,
                "total_examples": 80,
                "num_answered": 29
              },
              "PubMedQA_test": {
                "accuracy": 0.7649402390438247,
                "accuracy_ci": [
                  0.4820717131474104,
                  0.601593625498008
                ],
                "num_correct": 136,
                "total_examples": 251,
                "num_answered": 192
              }
            },
            "reasoning_4x_random": {
              "MedMCQA_validation": {
                "accuracy": 0.46924603174603174,
                "accuracy_ci": [
                  0.435515873015873,
                  0.5000248015873016
                ],
                "num_correct": 473,
                "total_examples": 1008,
                "num_answered": 469
              },
              "MedQA_USLME_test": {
                "accuracy": 0.5361842105263158,
                "accuracy_ci": [
                  0.48355263157894735,
                  0.5921052631578947
                ],
                "num_correct": 163,
                "total_examples": 304,
                "num_answered": 162
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.48739495798319327,
                "accuracy_ci": [
                  0.4369747899159664,
                  0.5406162464985994
                ],
                "num_correct": 174,
                "total_examples": 357,
                "num_answered": 172
              },
              "GPQA_Medical_test": {
                "accuracy": 0.4625,
                "accuracy_ci": [
                  0.3375,
                  0.55
                ],
                "num_correct": 35,
                "total_examples": 80,
                "num_answered": 37
              },
              "PubMedQA_test": {
                "accuracy": 0.6812749003984063,
                "accuracy_ci": [
                  0.6215139442231076,
                  0.7330677290836654
                ],
                "num_correct": 171,
                "total_examples": 251,
                "num_answered": 171
              }
            },
            "reasoning_2x_random": {
              "MedMCQA_validation": {
                "accuracy": 0.4732142857142857,
                "accuracy_ci": [
                  0.44047619047619047,
                  0.5039930555555555
                ],
                "num_correct": 477,
                "total_examples": 1008,
                "num_answered": 472
              },
              "MedQA_USLME_test": {
                "accuracy": 0.5526315789473685,
                "accuracy_ci": [
                  0.5,
                  0.6052631578947368
                ],
                "num_correct": 168,
                "total_examples": 304,
                "num_answered": 162
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5266106442577031,
                "accuracy_ci": [
                  0.47619047619047616,
                  0.5770308123249299
                ],
                "num_correct": 188,
                "total_examples": 357,
                "num_answered": 179
              },
              "GPQA_Medical_test": {
                "accuracy": 0.3125,
                "accuracy_ci": [
                  0.2,
                  0.4
                ],
                "num_correct": 24,
                "total_examples": 80,
                "num_answered": 25
              },
              "PubMedQA_test": {
                "accuracy": 0.6175298804780877,
                "accuracy_ci": [
                  0.5577689243027888,
                  0.6772908366533864
                ],
                "num_correct": 155,
                "total_examples": 251,
                "num_answered": 155
              }
            }
          },
          "reasoning_tokens": {
            "reasoning": 193.935,
            "reasoning_4x_random": 495.6855,
            "reasoning_2x_random": 331.6775
          },
          "test_time_scaling_plot": "/share/pi/nigam/users/calebwin/hf_cache/eval/base-tts/Llama-3.1-8B-Instructeval_data_plot.png"
        }
      }
    },
    "base-tts-wait": {
      "description": "Test time scaling evaluation of base model",
      "config": {
        "test_time_scaling": true,
        "method": "base",
        "eval_sample": 2000,
        "model_key": "llama3.1:8b",
        "reasoning_approaches": [
          "reasoning",
          "reasoning_2x",
          "reasoning_4x"
        ]
      },
      "results": {
        "training": {},
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/base-tts/Llama-3.1-8B-Instructeval_data.json",
          "timestamp": "2025-03-01T01:23:01.461759",
          "test_time_scaling": true,
          "summary": {
            "immediate": {
              "MedMCQA_validation": {
                "accuracy": 0.5476190476190477,
                "accuracy_ci": [
                  0.5178571428571429,
                  0.5764136904761904
                ],
                "num_correct": 552,
                "total_examples": 1008,
                "num_answered": 537
              },
              "MedQA_USLME_test": {
                "accuracy": 0.5855263157894737,
                "accuracy_ci": [
                  0.5296052631578947,
                  0.6414473684210527
                ],
                "num_correct": 178,
                "total_examples": 304,
                "num_answered": 124
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5490196078431373,
                "accuracy_ci": [
                  0.4957983193277311,
                  0.5966386554621849
                ],
                "num_correct": 196,
                "total_examples": 357,
                "num_answered": 192
              },
              "GPQA_Medical_test": {
                "accuracy": 0.3125,
                "accuracy_ci": [
                  0.2125,
                  0.4128124999999997
                ],
                "num_correct": 25,
                "total_examples": 80,
                "num_answered": 18
              },
              "PubMedQA_test": {
                "accuracy": 0.7649402390438247,
                "accuracy_ci": [
                  0.7091633466135459,
                  0.8167330677290837
                ],
                "num_correct": 192,
                "total_examples": 251,
                "num_answered": 192
              }
            },
            "reasoning": {
              "MedMCQA_validation": {
                "accuracy": 0.5426587301587301,
                "accuracy_ci": [
                  0.5128720238095238,
                  0.5714285714285714
                ],
                "num_correct": 547,
                "total_examples": 1008,
                "num_answered": 546
              },
              "MedQA_USLME_test": {
                "accuracy": 0.4901315789473684,
                "accuracy_ci": [
                  0.41776315789473684,
                  0.5230263157894737
                ],
                "num_correct": 142,
                "total_examples": 304,
                "num_answered": 149
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6134453781512605,
                "accuracy_ci": [
                  0.49299719887955185,
                  0.6050420168067226
                ],
                "num_correct": 196,
                "total_examples": 357,
                "num_answered": 219
              },
              "GPQA_Medical_test": {
                "accuracy": 0.45,
                "accuracy_ci": [
                  0.35,
                  0.55
                ],
                "num_correct": 36,
                "total_examples": 80,
                "num_answered": 34
              },
              "PubMedQA_test": {
                "accuracy": 0.7808764940239044,
                "accuracy_ci": [
                  0.5657370517928287,
                  0.6812749003984063
                ],
                "num_correct": 157,
                "total_examples": 251,
                "num_answered": 196
              }
            },
            "reasoning_2x": {
              "MedMCQA_validation": {
                "accuracy": 0.5307539682539683,
                "accuracy_ci": [
                  0.4999503968253969,
                  0.5605406746031746
                ],
                "num_correct": 535,
                "total_examples": 1008,
                "num_answered": 530
              },
              "MedQA_USLME_test": {
                "accuracy": 0.6085526315789473,
                "accuracy_ci": [
                  0.5493421052631579,
                  0.6644736842105263
                ],
                "num_correct": 185,
                "total_examples": 304,
                "num_answered": 181
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5742296918767507,
                "accuracy_ci": [
                  0.5210084033613446,
                  0.6246498599439776
                ],
                "num_correct": 205,
                "total_examples": 357,
                "num_answered": 196
              },
              "GPQA_Medical_test": {
                "accuracy": 0.475,
                "accuracy_ci": [
                  0.3625,
                  0.5875
                ],
                "num_correct": 38,
                "total_examples": 80,
                "num_answered": 38
              },
              "PubMedQA_test": {
                "accuracy": 0.7330677290836654,
                "accuracy_ci": [
                  0.6772908366533864,
                  0.7888446215139442
                ],
                "num_correct": 184,
                "total_examples": 251,
                "num_answered": 184
              }
            },
            "reasoning_4x": {
              "MedMCQA_validation": {
                "accuracy": 0.5406746031746031,
                "accuracy_ci": [
                  0.5049603174603174,
                  0.5684771825396825
                ],
                "num_correct": 540,
                "total_examples": 1008,
                "num_answered": 545
              },
              "MedQA_USLME_test": {
                "accuracy": 0.631578947368421,
                "accuracy_ci": [
                  0.569078947368421,
                  0.6842105263157895
                ],
                "num_correct": 192,
                "total_examples": 304,
                "num_answered": 190
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.5490196078431373,
                "accuracy_ci": [
                  0.49859943977591037,
                  0.6023109243697478
                ],
                "num_correct": 196,
                "total_examples": 357,
                "num_answered": 186
              },
              "GPQA_Medical_test": {
                "accuracy": 0.3875,
                "accuracy_ci": [
                  0.275,
                  0.4878124999999997
                ],
                "num_correct": 31,
                "total_examples": 80,
                "num_answered": 30
              },
              "PubMedQA_test": {
                "accuracy": 0.7410358565737052,
                "accuracy_ci": [
                  0.6892430278884463,
                  0.7928286852589641
                ],
                "num_correct": 186,
                "total_examples": 251,
                "num_answered": 186
              }
            }
          },
          "reasoning_tokens": {
            "immediate": 0,
            "reasoning": 191.0515,
            "reasoning_2x": 365.9285,
            "reasoning_4x": 605.6975
          },
          "test_time_scaling_plot": "/share/pi/nigam/users/calebwin/hf_cache/eval/base-tts/Llama-3.1-8B-Instructeval_data_plot.png"
        }
      }
    },
    "huatuo-tts": {
      "description": "Test time scaling evaluation of HuatuoGPT",
      "config": {
        "test_time_scaling": true,
        "method": "huatuo",
        "model_key": "huatuo:8b",
        "reasoning_approaches": [
          "reasoning",
          "reasoning_1x_random",
          "reasoning_2x_random",
          "reasoning_3x_random"
        ]
      },
      "results": {
        "training": {},
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo-tts/HuatuoGPT-o1-8Beval_data.json",
          "timestamp": "2025-03-04T10:07:08.385590",
          "test_time_scaling": true,
          "summary": {
            "reasoning_1x_random": {
              "MedMCQA_validation": {
                "accuracy": 0.6480994501553908,
                "accuracy_ci": [
                  0.6339947406167822,
                  0.663411427205355
                ],
                "num_correct": 2711,
                "total_examples": 4183,
                "num_answered": 2704
              },
              "MedQA_USLME_test": {
                "accuracy": 0.7525530243519246,
                "accuracy_ci": [
                  0.7289670070699137,
                  0.7761390416339355
                ],
                "num_correct": 958,
                "total_examples": 1273,
                "num_answered": 945
              },
              "PubMedQA_test": {
                "accuracy": 0.78,
                "accuracy_ci": [
                  0.752,
                  0.807
                ],
                "num_correct": 780,
                "total_examples": 1000,
                "num_answered": 778
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6390879478827362,
                "accuracy_ci": [
                  0.61628664495114,
                  0.6612540716612377
                ],
                "num_correct": 981,
                "total_examples": 1535,
                "num_answered": 971
              },
              "GPQA_Medical_test": {
                "accuracy": 0.5538461538461539,
                "accuracy_ci": [
                  0.49743589743589745,
                  0.5948717948717949
                ],
                "num_correct": 214,
                "total_examples": 390,
                "num_answered": 216
              }
            },
            "reasoning": {
              "MedMCQA_validation": {
                "accuracy": 0.634233803490318,
                "accuracy_ci": [
                  0.6184496772651207,
                  0.6473822615347836
                ],
                "num_correct": 2650,
                "total_examples": 4183,
                "num_answered": 2653
              },
              "MedQA_USLME_test": {
                "accuracy": 0.7714061272584446,
                "accuracy_ci": [
                  0.7486252945797329,
                  0.795758051846033
                ],
                "num_correct": 982,
                "total_examples": 1273,
                "num_answered": 971
              },
              "PubMedQA_test": {
                "accuracy": 0.791,
                "accuracy_ci": [
                  0.7649750000000001,
                  0.813
                ],
                "num_correct": 789,
                "total_examples": 1000,
                "num_answered": 791
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6482084690553745,
                "accuracy_ci": [
                  0.6175895765472312,
                  0.6657980456026059
                ],
                "num_correct": 985,
                "total_examples": 1535,
                "num_answered": 995
              },
              "GPQA_Medical_test": {
                "accuracy": 0.5794871794871795,
                "accuracy_ci": [
                  0.5332692307692308,
                  0.6282692307692307
                ],
                "num_correct": 226,
                "total_examples": 390,
                "num_answered": 224
              }
            },
            "reasoning_2x_random": {
              "MedMCQA_validation": {
                "accuracy": 0.6311259861343533,
                "accuracy_ci": [
                  0.616782213722209,
                  0.6457147979918719
                ],
                "num_correct": 2640,
                "total_examples": 4183,
                "num_answered": 2616
              },
              "MedQA_USLME_test": {
                "accuracy": 0.7470542026708562,
                "accuracy_ci": [
                  0.7258248232521604,
                  0.7714061272584446
                ],
                "num_correct": 951,
                "total_examples": 1273,
                "num_answered": 933
              },
              "PubMedQA_test": {
                "accuracy": 0.78,
                "accuracy_ci": [
                  0.753,
                  0.804
                ],
                "num_correct": 779,
                "total_examples": 1000,
                "num_answered": 780
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6286644951140065,
                "accuracy_ci": [
                  0.6058631921824105,
                  0.6521172638436482
                ],
                "num_correct": 965,
                "total_examples": 1535,
                "num_answered": 961
              },
              "GPQA_Medical_test": {
                "accuracy": 0.558974358974359,
                "accuracy_ci": [
                  0.5102564102564102,
                  0.6051282051282051
                ],
                "num_correct": 218,
                "total_examples": 390,
                "num_answered": 217
              }
            },
            "reasoning_3x_random": {
              "MedMCQA_validation": {
                "accuracy": 0.6273009801577815,
                "accuracy_ci": [
                  0.6124790819985656,
                  0.6418897920153
                ],
                "num_correct": 2624,
                "total_examples": 4183,
                "num_answered": 2597
              },
              "MedQA_USLME_test": {
                "accuracy": 0.753338570306363,
                "accuracy_ci": [
                  0.7274155538098979,
                  0.7753338570306363
                ],
                "num_correct": 958,
                "total_examples": 1273,
                "num_answered": 959
              },
              "PubMedQA_test": {
                "accuracy": 0.778,
                "accuracy_ci": [
                  0.752,
                  0.802
                ],
                "num_correct": 778,
                "total_examples": 1000,
                "num_answered": 778
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6423452768729642,
                "accuracy_ci": [
                  0.6188925081433225,
                  0.6651465798045603
                ],
                "num_correct": 986,
                "total_examples": 1535,
                "num_answered": 975
              },
              "GPQA_Medical_test": {
                "accuracy": 0.5230769230769231,
                "accuracy_ci": [
                  0.4717948717948718,
                  0.5692307692307692
                ],
                "num_correct": 203,
                "total_examples": 390,
                "num_answered": 204
              }
            }
          },
          "reasoning_tokens": {
            "reasoning_1x_random": 720.3079584775087,
            "reasoning": 485.34494690371076,
            "reasoning_2x_random": 1015.953108220976,
            "reasoning_3x_random": 1308.074334804916
          },
          "test_time_scaling_plot": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo-tts/HuatuoGPT-o1-8Beval_data_plot.png"
        }
      }
    },
    "huatuo-tts-wait": {
      "description": "Test time scaling evaluation of HuatuoGPT",
      "config": {
        "test_time_scaling": true,
        "method": "huatuo",
        "model_key": "huatuo:8b",
        "reasoning_approaches": [
          "reasoning",
          "reasoning_1x",
          "reasoning_2x",
          "reasoning_3x"
        ]
      },
      "results": {
        "training": {},
        "eval": {
          "outputs_path": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo-tts-wait/HuatuoGPT-o1-8Beval_data.json",
          "timestamp": "2025-03-04T10:00:05.776517",
          "test_time_scaling": true,
          "summary": {
            "reasoning_3x": {
              "MedMCQA_validation": {
                "accuracy": 0.6316041118814248,
                "accuracy_ci": [
                  0.6163040879751375,
                  0.6471431986612479
                ],
                "num_correct": 2642,
                "total_examples": 4183,
                "num_answered": 2641
              },
              "MedQA_USLME_test": {
                "accuracy": 0.7478397486252946,
                "accuracy_ci": [
                  0.7266103692065986,
                  0.7729968578161822
                ],
                "num_correct": 952,
                "total_examples": 1273,
                "num_answered": 942
              },
              "PubMedQA_test": {
                "accuracy": 0.784,
                "accuracy_ci": [
                  0.755,
                  0.806
                ],
                "num_correct": 780,
                "total_examples": 1000,
                "num_answered": 784
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6390879478827362,
                "accuracy_ci": [
                  0.6143159609120521,
                  0.6644951140065146
                ],
                "num_correct": 981,
                "total_examples": 1535,
                "num_answered": 957
              },
              "GPQA_Medical_test": {
                "accuracy": 0.5487179487179488,
                "accuracy_ci": [
                  0.49743589743589745,
                  0.5949358974358974
                ],
                "num_correct": 213,
                "total_examples": 390,
                "num_answered": 214
              }
            },
            "reasoning_1x": {
              "MedMCQA_validation": {
                "accuracy": 0.6392541238345685,
                "accuracy_ci": [
                  0.6249103514224241,
                  0.6540819985656228
                ],
                "num_correct": 2674,
                "total_examples": 4183,
                "num_answered": 2668
              },
              "MedQA_USLME_test": {
                "accuracy": 0.7494108405341713,
                "accuracy_ci": [
                  0.7266300078554595,
                  0.7729772191673213
                ],
                "num_correct": 954,
                "total_examples": 1273,
                "num_answered": 938
              },
              "PubMedQA_test": {
                "accuracy": 0.787,
                "accuracy_ci": [
                  0.76,
                  0.810025
                ],
                "num_correct": 786,
                "total_examples": 1000,
                "num_answered": 787
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6429967426710098,
                "accuracy_ci": [
                  0.6195439739413681,
                  0.6677687296416938
                ],
                "num_correct": 987,
                "total_examples": 1535,
                "num_answered": 973
              },
              "GPQA_Medical_test": {
                "accuracy": 0.5564102564102564,
                "accuracy_ci": [
                  0.5,
                  0.5974999999999999
                ],
                "num_correct": 215,
                "total_examples": 390,
                "num_answered": 217
              }
            },
            "reasoning_2x": {
              "MedMCQA_validation": {
                "accuracy": 0.6371025579727468,
                "accuracy_ci": [
                  0.6227587855606025,
                  0.6514463303848912
                ],
                "num_correct": 2665,
                "total_examples": 4183,
                "num_answered": 2660
              },
              "MedQA_USLME_test": {
                "accuracy": 0.779261586802828,
                "accuracy_ci": [
                  0.7564807541241163,
                  0.8020424194815396
                ],
                "num_correct": 992,
                "total_examples": 1273,
                "num_answered": 982
              },
              "PubMedQA_test": {
                "accuracy": 0.783,
                "accuracy_ci": [
                  0.76,
                  0.809
                ],
                "num_correct": 783,
                "total_examples": 1000,
                "num_answered": 783
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6521172638436482,
                "accuracy_ci": [
                  0.6260586319218241,
                  0.6755700325732898
                ],
                "num_correct": 1001,
                "total_examples": 1535,
                "num_answered": 975
              },
              "GPQA_Medical_test": {
                "accuracy": 0.5384615384615384,
                "accuracy_ci": [
                  0.4897435897435897,
                  0.5846153846153846
                ],
                "num_correct": 210,
                "total_examples": 390,
                "num_answered": 209
              }
            },
            "reasoning": {
              "MedMCQA_validation": {
                "accuracy": 0.643318192684676,
                "accuracy_ci": [
                  0.6299306717666746,
                  0.6579010279703562
                ],
                "num_correct": 2691,
                "total_examples": 4183,
                "num_answered": 2673
              },
              "MedQA_USLME_test": {
                "accuracy": 0.7666928515318147,
                "accuracy_ci": [
                  0.7439120188531029,
                  0.7894736842105263
                ],
                "num_correct": 976,
                "total_examples": 1273,
                "num_answered": 970
              },
              "PubMedQA_test": {
                "accuracy": 0.798,
                "accuracy_ci": [
                  0.772,
                  0.8210249999999999
                ],
                "num_correct": 797,
                "total_examples": 1000,
                "num_answered": 798
              },
              "MMLU-Pro_Medical_test": {
                "accuracy": 0.6403908794788273,
                "accuracy_ci": [
                  0.6175895765472312,
                  0.6644951140065146
                ],
                "num_correct": 983,
                "total_examples": 1535,
                "num_answered": 976
              },
              "GPQA_Medical_test": {
                "accuracy": 0.5846153846153846,
                "accuracy_ci": [
                  0.5205128205128206,
                  0.617948717948718
                ],
                "num_correct": 222,
                "total_examples": 390,
                "num_answered": 228
              }
            }
          },
          "reasoning_tokens": {
            "reasoning_3x": 926.2552201407947,
            "reasoning_1x": 558.876983653502,
            "reasoning_2x": 746.5506502803961,
            "reasoning": 485.294952869586
          },
          "test_time_scaling_plot": "/share/pi/nigam/users/calebwin/hf_cache/eval/huatuo-tts-wait/HuatuoGPT-o1-8Beval_data_plot.png"
        }
      }
    }
  }
}
